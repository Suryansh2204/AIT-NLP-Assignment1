{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec (Skipgram )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package semcor to\n",
      "[nltk_data]     C:\\Users\\surya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\surya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import semcor, stopwords\n",
    "\n",
    "nltk.download('semcor')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA GeForce RTX 2080 with Max-Q Design')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor.words()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. tokenization\n",
    "corpus = semcor.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "corpus = [[word for word in sent if word.lower() not in stop_words] for sent in corpus]\n",
    "\n",
    "# Remove punctuation from corpus\n",
    "from string import punctuation\n",
    "corpus = [[word for word in sent if word not in punctuation] for sent in corpus]\n",
    "\n",
    "# Remove empty sentences\n",
    "corpus = [sent for sent in corpus if len(sent) > 0]\n",
    "\n",
    "# Remove sentences with less than 5 words\n",
    "corpus = [sent for sent in corpus if len(sent) >= 5]\n",
    "\n",
    "# Remove sentences with more than 20 words\n",
    "corpus = [sent for sent in corpus if len(sent) <= 20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. numeralization\n",
    "#find unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "#assign unique integer\n",
    "vocabs = list(set(flatten(corpus))) #all the words we have in the system - <UNK>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create handy mapping between integer and word\n",
    "word2index = {v:idx for idx, v in enumerate(vocabs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = len(vocabs) - 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pairs of center word, and outside word\n",
    "\n",
    "def random_batch(batch_size, corpus, window_size=2):\n",
    "\n",
    "    skipgrams = []\n",
    "\n",
    "    #loop each corpus\n",
    "    for doc in corpus:\n",
    "        #look from the 2nd word until second last word\n",
    "        for i in range(window_size, len(doc)-window_size):\n",
    "            #center word\n",
    "            center = word2index[doc[i]]\n",
    "            \n",
    "            #outside words (window size on both sides of the center word)\n",
    "            outside_start =  i - window_size\n",
    "            outside_end =  i + window_size + 1 # +1 because the end index is exclusive\n",
    "\n",
    "            # outside = []\n",
    "            # loop through the outside words, append to the list 'outside'\n",
    "            for j in range(outside_start, outside_end):\n",
    "                if i != j:  # Skip the center word\n",
    "                    outside= word2index[doc[j]]\n",
    "                    skipgrams.append([center, outside])\n",
    "\n",
    "                \n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "    \n",
    "    inputs, labels = [], []\n",
    "    for index in random_index:\n",
    "        inputs.append([skipgrams[index][0]])\n",
    "        labels.append([skipgrams[index][1]])\n",
    "        \n",
    "    return np.array(inputs), np.array(labels)\n",
    "            \n",
    "x, y = random_batch(2, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25398],\n",
       "       [ 2257]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape  #batch_size 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\\log P(w_{t+j} | w_t; \\theta)$$\n",
    "\n",
    "where $P(w_{t+j} | w_t; \\theta) = $\n",
    "\n",
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$\n",
    "\n",
    "where $o$ is the outside words and $c$ is the center word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center)  #(batch_size, 1, emb_size)\n",
    "        outside_embedding    = self.embedding_center(outside) #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embedding = self.embedding_center(all_vocabs) #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        #batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1) \n",
    "\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        #batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size) \n",
    "        \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)  #(batch_size, 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))  #scalar\n",
    "        \n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ..., 36637, 36638, 36639],\n",
       "        [    0,     1,     2,  ..., 36637, 36638, 36639]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare all vocabs\n",
    "\n",
    "batch_size = 2\n",
    "voc_size   = len(vocabs)\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skipgram(\n",
       "  (embedding_center): Embedding(36640, 2)\n",
       "  (embedding_outside): Embedding(36640, 2)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Skipgram(voc_size, 2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.LongTensor(x)\n",
    "label_tensor = torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(input_tensor, label_tensor, all_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.7479, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "emb_size   = 2\n",
    "model      = Skipgram(voc_size, emb_size)\n",
    "optimizer  = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def log_epoch(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | Loss: 12.330386 | Time: 1m 5s\n",
      "Epoch: 200 | Loss: 10.850671 | Time: 2m 11s\n",
      "Epoch: 300 | Loss: 11.087509 | Time: 3m 22s\n",
      "Epoch: 400 | Loss: 14.636918 | Time: 4m 34s\n",
      "Epoch: 500 | Loss: 11.455265 | Time: 5m 40s\n",
      "Epoch: 600 | Loss: 9.962486 | Time: 6m 50s\n",
      "Epoch: 700 | Loss: 12.481160 | Time: 7m 59s\n",
      "Epoch: 800 | Loss: 10.493835 | Time: 9m 10s\n",
      "Epoch: 900 | Loss: 12.907803 | Time: 10m 19s\n",
      "Epoch: 1000 | Loss: 11.106666 | Time: 11m 29s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "num_epochs = 1000\n",
    "window_size = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus, window_size)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    #predict\n",
    "    loss = model(input_tensor, label_tensor, all_vocabs)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    # Log epoch time\n",
    "    epoch_mins, epoch_secs = log_epoch(start, time.time())\n",
    "    \n",
    "    #print the loss\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | Loss: {loss:.6f} | Time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot the embeddings\n",
    "\n",
    "Is fruit really near to banana?\n",
    "Is fruit really far from cat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recluse',\n",
       " 'unlamented',\n",
       " 'soldiery',\n",
       " 'commonwealth',\n",
       " 'Gettysburg',\n",
       " 'stuck-up',\n",
       " 'groove',\n",
       " 'maria',\n",
       " 'free',\n",
       " 'mates',\n",
       " 'Brittany',\n",
       " 'Hunters',\n",
       " 'stay',\n",
       " 'blaze',\n",
       " 'custodian',\n",
       " 'replied',\n",
       " 'entrenched',\n",
       " 'allowances',\n",
       " 'understood',\n",
       " 'unreflective',\n",
       " 'stagnation',\n",
       " 'Ziminska-Sygietynska',\n",
       " 'obscures',\n",
       " 'misbehavior',\n",
       " 'Rebel',\n",
       " 'impasse',\n",
       " 'driver',\n",
       " 'speeds',\n",
       " 'bunt',\n",
       " 'exhibiting',\n",
       " 'incipience',\n",
       " 'assaulted',\n",
       " 'statutes',\n",
       " 'Spahn',\n",
       " 'experimented',\n",
       " 'Forum',\n",
       " '1863',\n",
       " 'warfront',\n",
       " 'controversy',\n",
       " 'indirect',\n",
       " 'snowfall',\n",
       " 'Nowadays',\n",
       " 'Durlach',\n",
       " 'independence',\n",
       " 'Quell',\n",
       " 'Lehmann',\n",
       " 'parisology',\n",
       " 'Phoenix',\n",
       " 'underside',\n",
       " 'life',\n",
       " 'legged',\n",
       " 'archives',\n",
       " 'dynasty',\n",
       " 'autism',\n",
       " 'shoelace',\n",
       " 'Tradition',\n",
       " 'materiel',\n",
       " 'airmen',\n",
       " 'distinction',\n",
       " 'Racine',\n",
       " 'refused',\n",
       " 'totally',\n",
       " 'vivo',\n",
       " 'Rolls',\n",
       " 'vivify',\n",
       " 'Mister',\n",
       " \"mind's\",\n",
       " 'multipurpose',\n",
       " 'Micelles',\n",
       " 'Junction',\n",
       " 'Grand',\n",
       " 'billets',\n",
       " 'inane',\n",
       " 'elucidated',\n",
       " 'Bulletin',\n",
       " 'disproving',\n",
       " 'ugliness',\n",
       " 'tranquilizers',\n",
       " 'snag',\n",
       " 'debt-free',\n",
       " 'interpreted',\n",
       " 'leasing',\n",
       " 'ocher',\n",
       " 'evangelism',\n",
       " 'announcer',\n",
       " 'Palomar',\n",
       " 'Potlatches',\n",
       " 'evidencing',\n",
       " 'graphed',\n",
       " 'anti-French',\n",
       " 'Counsel',\n",
       " 'Doughnuttery',\n",
       " 'refractory',\n",
       " 'mettle',\n",
       " 'Knowledge',\n",
       " 'dismal',\n",
       " 'scholastics',\n",
       " 'dad',\n",
       " 'Sampson',\n",
       " 'noticeably',\n",
       " 'shameful',\n",
       " 'August',\n",
       " 'averted',\n",
       " 'Greatest',\n",
       " 'rackets',\n",
       " 'paramilitary',\n",
       " 'chanted',\n",
       " 'strife',\n",
       " 'outsider',\n",
       " 'deferring',\n",
       " 'livers',\n",
       " 'playmate',\n",
       " 'electing',\n",
       " '3:30',\n",
       " 'unimpeachably',\n",
       " 'Culver',\n",
       " 'competence',\n",
       " 'ornamented',\n",
       " 'push',\n",
       " 'microfossils',\n",
       " '0.2',\n",
       " 'inauguration',\n",
       " 'socks',\n",
       " 'Dimes',\n",
       " 'Jerebohms',\n",
       " 'SAAMI',\n",
       " 'fillies',\n",
       " 'to-day',\n",
       " 'Vermejo',\n",
       " 'Denouncing',\n",
       " 'thine',\n",
       " 'Unitarian',\n",
       " '2.16',\n",
       " 'Somewhere',\n",
       " 'delicacies',\n",
       " 'chaos',\n",
       " 'Noticing',\n",
       " 'stabilization',\n",
       " 'Greece',\n",
       " 'red',\n",
       " 'crash',\n",
       " 'parasites',\n",
       " 'Neurenschatz',\n",
       " 'directors',\n",
       " 'denuded',\n",
       " 'thorny',\n",
       " 'advised',\n",
       " 'summary',\n",
       " 'void',\n",
       " 'Exposition',\n",
       " 'vine',\n",
       " 'Flaming',\n",
       " 'Drawn',\n",
       " 'Leland',\n",
       " 'deed',\n",
       " 'Banion',\n",
       " 'Charter',\n",
       " 'beep',\n",
       " 'brainy',\n",
       " 'reminded',\n",
       " 'objection',\n",
       " 'well-received',\n",
       " 'goings',\n",
       " 'royalty',\n",
       " 'Arden',\n",
       " 'leapt',\n",
       " 'reorganizing',\n",
       " 'fluffy',\n",
       " 'luckier',\n",
       " 'relativity',\n",
       " 'good-looking',\n",
       " 'Acapulco',\n",
       " 'saddlebags',\n",
       " 'Grassy',\n",
       " 'Salter',\n",
       " 'Ashamed',\n",
       " 'afternoons',\n",
       " 'Master',\n",
       " 'Compress',\n",
       " 'Axis',\n",
       " 'Universe',\n",
       " 'gift',\n",
       " 'takeover',\n",
       " 'spirito',\n",
       " 'Exclusive',\n",
       " 'Indication',\n",
       " 'irredentism',\n",
       " 'hearty',\n",
       " 'treacherous',\n",
       " 'cramp',\n",
       " 'honeybee',\n",
       " 'baffling',\n",
       " '5835',\n",
       " 'palm-studded',\n",
       " 'Gustaf',\n",
       " \"Lord's\",\n",
       " 'varieties',\n",
       " 'occupancies',\n",
       " 'dramatics',\n",
       " 'hauls',\n",
       " 'sorting',\n",
       " 'pine-knot',\n",
       " 'ounce',\n",
       " 'underground',\n",
       " 'engulfs',\n",
       " 'vice-president',\n",
       " 'Latter-day',\n",
       " '720',\n",
       " 'Sokolov',\n",
       " 'guards',\n",
       " 'fjords',\n",
       " 'malaise',\n",
       " 'translate',\n",
       " 'surface-active',\n",
       " 'bulky',\n",
       " 'metabolic',\n",
       " 'Andrus',\n",
       " 'sleepily',\n",
       " 'Resolving',\n",
       " 'underwriting',\n",
       " 'Chairman',\n",
       " 'Farneses',\n",
       " 'Hans',\n",
       " 'loveliness',\n",
       " 'v',\n",
       " 'journalism',\n",
       " 'profuse',\n",
       " 'Garland',\n",
       " 'concessions',\n",
       " 'Catholics',\n",
       " 'whole-wheat',\n",
       " 'expiration',\n",
       " 'calm',\n",
       " 'Stardel',\n",
       " 'repelled',\n",
       " 'kilometers',\n",
       " 'Jean',\n",
       " 'hookup',\n",
       " 'invitations',\n",
       " 'See-through',\n",
       " 'Cortlandt',\n",
       " 'journalists',\n",
       " 'Jimmy',\n",
       " 'besmirched',\n",
       " 'Mustard',\n",
       " 'easiest',\n",
       " 'forcing',\n",
       " 'inducing',\n",
       " 'exchanges',\n",
       " 'asymptotic',\n",
       " 'free-lance',\n",
       " 'fluently',\n",
       " 'Spice',\n",
       " 'pitiful',\n",
       " 'supreme',\n",
       " 'Institutes',\n",
       " 'consciously',\n",
       " 'attentive',\n",
       " 'histology',\n",
       " 'stilts',\n",
       " 'rawhide',\n",
       " 'enjoyment',\n",
       " 'digging',\n",
       " 'intensive',\n",
       " 'colonel',\n",
       " 'plotting',\n",
       " 'anticipated',\n",
       " 'Originally',\n",
       " 'glancing',\n",
       " 'Repertory',\n",
       " 'pitchfork',\n",
       " 'approximately',\n",
       " 'Stetson',\n",
       " 'horse-radish',\n",
       " 'turbulence',\n",
       " 'violin',\n",
       " 'conjectures',\n",
       " 'side-step',\n",
       " 'curative',\n",
       " 'boundary',\n",
       " 'brandy',\n",
       " 'Buddha',\n",
       " 'recounted',\n",
       " 'yelling',\n",
       " 'salvaging',\n",
       " 'preparedness',\n",
       " 'huddle',\n",
       " 'reputation',\n",
       " 'expeditiously',\n",
       " 'prowl',\n",
       " 'Ambiguity',\n",
       " 'reviewed',\n",
       " 'rodeo',\n",
       " 'hoop',\n",
       " 'blemishes',\n",
       " 'temperament',\n",
       " 'Gasoline',\n",
       " 'countryman',\n",
       " 'banisters',\n",
       " 'Staley',\n",
       " 'Wales',\n",
       " 'unending',\n",
       " 'sine',\n",
       " 'closely',\n",
       " 'phonology',\n",
       " 'drew',\n",
       " 'Composite',\n",
       " 'unknown',\n",
       " 'Norell',\n",
       " 'wanderings',\n",
       " 'nonreactors',\n",
       " 'punishing',\n",
       " 'coffee-cup',\n",
       " 'Christmas',\n",
       " 'bit-like',\n",
       " 'option',\n",
       " 'lock',\n",
       " 'Buell',\n",
       " 'borrower',\n",
       " 'Hedda',\n",
       " 'crossbars',\n",
       " 'prepared',\n",
       " 'Earl',\n",
       " 'conjunctions',\n",
       " 'behaviorally',\n",
       " 'Putting',\n",
       " 'glassy',\n",
       " 'diverging',\n",
       " 'yearnings',\n",
       " 'flappers',\n",
       " 'chill',\n",
       " 'horoscope',\n",
       " 'assemblage',\n",
       " 'smartly',\n",
       " 'wondered',\n",
       " 'high-powered',\n",
       " 'adapters',\n",
       " 'nothing',\n",
       " 'Smith-Hughes',\n",
       " 'gesture',\n",
       " 'abolish',\n",
       " 'nonreactivity',\n",
       " 'superficiality',\n",
       " 'congealed',\n",
       " 'Could',\n",
       " 'Suez-Hungary',\n",
       " 'clamshell',\n",
       " 'observes',\n",
       " 'Afterwards',\n",
       " 'evangelical',\n",
       " 'cypress',\n",
       " 'journalist',\n",
       " 'buckshot',\n",
       " 'Paradoxically',\n",
       " 'anticipates',\n",
       " 'allergies',\n",
       " 'therefor',\n",
       " 'highway',\n",
       " 'Gloriana',\n",
       " 'tidying',\n",
       " 'mediocrity',\n",
       " 'Howser',\n",
       " 'creations',\n",
       " 'monitoring',\n",
       " 'coatings',\n",
       " 'CDC',\n",
       " 'Pullover',\n",
       " 'Discourse',\n",
       " 'overcast',\n",
       " 'strokes',\n",
       " 'unfailing',\n",
       " 'Lectures',\n",
       " 'racket',\n",
       " 'Speedup',\n",
       " '901',\n",
       " 'contraception',\n",
       " 'biggest',\n",
       " 'Abstractions',\n",
       " 'exactly',\n",
       " 'complicity',\n",
       " 'Coulson',\n",
       " 'produce',\n",
       " 'shims',\n",
       " 'decay',\n",
       " 'radionic',\n",
       " 'fanciful',\n",
       " 'bibliographies',\n",
       " 'consisting',\n",
       " 'indivisibility',\n",
       " 'peddler',\n",
       " 'television',\n",
       " 'hypnotically',\n",
       " 'unfamiliar',\n",
       " 'controls',\n",
       " 'lash',\n",
       " 'bullyboys',\n",
       " 'anhemolyticus',\n",
       " 'blindfolded',\n",
       " 'blinds',\n",
       " 'ruthlessness',\n",
       " 'pinch',\n",
       " 'crests',\n",
       " 'prophets',\n",
       " 'physiotherapist',\n",
       " 'Clair',\n",
       " 'blue-uniformed',\n",
       " 'according',\n",
       " 'seventeenth',\n",
       " 'Valentine',\n",
       " '20',\n",
       " 'circulate',\n",
       " 'Leon',\n",
       " 'commendation',\n",
       " 'Sulcer',\n",
       " 'sonata',\n",
       " 'shoot-down',\n",
       " 'epic',\n",
       " 'Y-regions',\n",
       " 'continuities',\n",
       " 'obligation',\n",
       " 'builder',\n",
       " 'kissing',\n",
       " 'difference',\n",
       " 'shun',\n",
       " 'ice-chest',\n",
       " 'hatching',\n",
       " 'spinach',\n",
       " 'antibody',\n",
       " 'unbounded',\n",
       " 'complicated',\n",
       " 'neighborhood',\n",
       " 'aiding',\n",
       " 'songful',\n",
       " 'started',\n",
       " 'disappointed',\n",
       " 'holystones',\n",
       " 'Andy',\n",
       " 'Sante',\n",
       " 'Bundy',\n",
       " 'Heinzes',\n",
       " 'writ',\n",
       " 'cautiously',\n",
       " 'Griffin',\n",
       " 'part',\n",
       " 'discriminating',\n",
       " 'synthesizes',\n",
       " '37500',\n",
       " 'minister',\n",
       " 'dissenter',\n",
       " 'utterance',\n",
       " 'Niobe',\n",
       " 'misinterpretation',\n",
       " 'dissimilar',\n",
       " 'sinful',\n",
       " 'Herblock',\n",
       " 'Round',\n",
       " 'resilience',\n",
       " 'yearningly',\n",
       " 'clippings',\n",
       " 'pranks',\n",
       " 'msec.',\n",
       " 'Mill',\n",
       " 'priming',\n",
       " 'commendable',\n",
       " 'hoodlum',\n",
       " 'involuntary-control',\n",
       " 'quit',\n",
       " 'well-stocked',\n",
       " 'internationalist',\n",
       " 'particularistic',\n",
       " '47.1',\n",
       " 'moderate-income',\n",
       " 'skylights',\n",
       " 'clashes',\n",
       " 'rocklike',\n",
       " 'forsakes',\n",
       " 'Sartre',\n",
       " 'semicircular',\n",
       " 'goose',\n",
       " 'inception',\n",
       " 'Sixty',\n",
       " 'Grudges',\n",
       " 'hunt',\n",
       " 'Vroman',\n",
       " 'faithful',\n",
       " 'rolls',\n",
       " '1980',\n",
       " 'dresser',\n",
       " 'bravura',\n",
       " 'willowy',\n",
       " 'facilitated',\n",
       " 'Really',\n",
       " 'Gansevoort',\n",
       " 'persistent',\n",
       " 'Davis',\n",
       " 'humanistic',\n",
       " 'Shahn',\n",
       " 'Schraffts',\n",
       " 'reflex',\n",
       " 'develops',\n",
       " 'tottering',\n",
       " 'Darrow',\n",
       " 'synagogue',\n",
       " 'subservient',\n",
       " 'viability',\n",
       " 'going',\n",
       " 'fought',\n",
       " 'statesman',\n",
       " 'speechlessness',\n",
       " 'Mortar',\n",
       " 'encompasses',\n",
       " 'Jessy',\n",
       " 'infielder',\n",
       " 'completely',\n",
       " 'redoubled',\n",
       " 'hydraulics',\n",
       " '2489000',\n",
       " 'institutionalized',\n",
       " 'Additionally',\n",
       " 'lyric',\n",
       " 'Quality',\n",
       " 'Fingered',\n",
       " 'objector',\n",
       " 'Sanctuary',\n",
       " 'venomous',\n",
       " 'lumbered',\n",
       " 'flanked',\n",
       " 'sod',\n",
       " 'shorts',\n",
       " 'look',\n",
       " 'ornate',\n",
       " 'Podger',\n",
       " 'worshippers',\n",
       " 'opportunities',\n",
       " 'Heiser',\n",
       " 're-run',\n",
       " 'season',\n",
       " 'pensioner',\n",
       " 'trap',\n",
       " 'drawings',\n",
       " 'Geometric',\n",
       " 'contradictorily',\n",
       " 'backgrounds',\n",
       " 'love-making',\n",
       " 'Epitaph',\n",
       " 'Noskova',\n",
       " 'gateways',\n",
       " 'outplayed',\n",
       " 'berry',\n",
       " 'H.',\n",
       " 'guts',\n",
       " 'Spencer',\n",
       " 'auxiliary',\n",
       " 'polynomial',\n",
       " 'sticks',\n",
       " 'Ghana',\n",
       " 'fountainhead',\n",
       " 'colorful',\n",
       " 'bunched',\n",
       " 'Bonjour',\n",
       " 'encrusted',\n",
       " 'boil',\n",
       " 'rundown',\n",
       " 'ibn',\n",
       " 'Edison',\n",
       " 'smoothly',\n",
       " 'Hits',\n",
       " 'reproducing',\n",
       " 'obligingly',\n",
       " 'hare',\n",
       " 'manifestly',\n",
       " 'woman',\n",
       " 'settlers',\n",
       " 'status',\n",
       " 'freed',\n",
       " 'North',\n",
       " 'Pentagon',\n",
       " 'torn',\n",
       " 'Subsequent',\n",
       " 'less-developed',\n",
       " 'pondered',\n",
       " 'asunder',\n",
       " 'Titus',\n",
       " 'Sweeneys',\n",
       " 'g.',\n",
       " 'iodotyrosines',\n",
       " 'birth-prevention',\n",
       " 'revetments',\n",
       " 'credible',\n",
       " 'rests',\n",
       " 'nay',\n",
       " '155',\n",
       " 'vicarious',\n",
       " 'barbs',\n",
       " 'shuns',\n",
       " 'Robert',\n",
       " 'froze',\n",
       " 'believers',\n",
       " 'sites',\n",
       " 'Briggs',\n",
       " 'pacers',\n",
       " 'parasite',\n",
       " 'nineties',\n",
       " 'anti-liquor',\n",
       " 'covetousness',\n",
       " 'sow',\n",
       " 'Edwards',\n",
       " '2.1',\n",
       " 'smallish',\n",
       " 'universalistic',\n",
       " 'Frozen',\n",
       " 'exalted',\n",
       " 'skeptically',\n",
       " 'horsemanship',\n",
       " 'Dimensions',\n",
       " 'another-the',\n",
       " 'inexperience',\n",
       " 'Vitus',\n",
       " 'concise',\n",
       " 'magnificence',\n",
       " 'cooing',\n",
       " 'pitched',\n",
       " 'sympathetic',\n",
       " 'correct',\n",
       " '450000',\n",
       " 'Hampton',\n",
       " 'idols',\n",
       " 'Hopefully',\n",
       " 'Notte',\n",
       " 'babies',\n",
       " 'figurative',\n",
       " 'Politburo',\n",
       " 'kingdom-wide',\n",
       " 'craze',\n",
       " 'Hendl',\n",
       " 'Fathers',\n",
       " 'gristmill',\n",
       " 'Setting',\n",
       " 'divert',\n",
       " 'sneers',\n",
       " 'Separating',\n",
       " 'fancy-free',\n",
       " 'revise',\n",
       " 'insinuations',\n",
       " 'dependent',\n",
       " 'snake-rail',\n",
       " 'Angeles-Pasadena',\n",
       " 'Surrounding',\n",
       " 'trumpet',\n",
       " 'skiis',\n",
       " 'Bride',\n",
       " 'fervently',\n",
       " 'Westerners',\n",
       " 'baseballs',\n",
       " 'Marcus',\n",
       " 'cloud',\n",
       " 'choreography',\n",
       " 'upper',\n",
       " 'Though',\n",
       " 'Swinburne',\n",
       " 'unsuited',\n",
       " 'ratiocinating',\n",
       " 'hypothalamus',\n",
       " 'Shipments',\n",
       " 're-evaluation',\n",
       " 'Records',\n",
       " 'justifiable',\n",
       " 'Benson',\n",
       " 'spur',\n",
       " 'domination',\n",
       " 'rare',\n",
       " 'hyperbole',\n",
       " 'comas',\n",
       " 'Peaceful',\n",
       " 'Torrio',\n",
       " 'bantered',\n",
       " 'bidders',\n",
       " 'forgave',\n",
       " 'sleeps',\n",
       " 'mutational',\n",
       " 'keeping',\n",
       " 'Hotels',\n",
       " 'maelstrom',\n",
       " 'Doors',\n",
       " '2170',\n",
       " 'mudslinging',\n",
       " 'ultrasonically',\n",
       " 'expansiveness',\n",
       " 'woolgather',\n",
       " 'boun',\n",
       " 'white-stucco',\n",
       " 'mythological',\n",
       " 'reluctant',\n",
       " 'jealous',\n",
       " 'emptiness',\n",
       " 'marginality',\n",
       " 'engaged',\n",
       " 'chilling',\n",
       " 'heard',\n",
       " 'plugged',\n",
       " 'low-class',\n",
       " 'Reconstruction',\n",
       " 'aviation',\n",
       " 'DES',\n",
       " 'tailor-make',\n",
       " 'theorem',\n",
       " 'penciled',\n",
       " 'Deductible',\n",
       " 'scouting',\n",
       " 'Raton',\n",
       " 'President-elect',\n",
       " 'commando',\n",
       " 'sympathy',\n",
       " 'incise',\n",
       " 'sweet',\n",
       " 'Shape',\n",
       " 'Yokel',\n",
       " 'Leverku^hn',\n",
       " 'short-skirted',\n",
       " 'pleasant',\n",
       " 'keening',\n",
       " 'obelisk',\n",
       " 'picnic',\n",
       " 'sneaks',\n",
       " 'idioms',\n",
       " 'nullity',\n",
       " 'Lights',\n",
       " 'describe',\n",
       " 'emission',\n",
       " 'joining',\n",
       " 'executing',\n",
       " 'anthropomorphic',\n",
       " 'speaks',\n",
       " 'peered',\n",
       " 'autos',\n",
       " '1857',\n",
       " 'interpretor',\n",
       " 'sable',\n",
       " 'Innesfree',\n",
       " 'Hayek',\n",
       " 'trichloroacetic',\n",
       " 'bequests',\n",
       " 'industrialists',\n",
       " 'recruiter',\n",
       " 'Growth',\n",
       " 'camper',\n",
       " 'shoving',\n",
       " 'thaw',\n",
       " '2490',\n",
       " '39000',\n",
       " 'communion',\n",
       " 'pillared',\n",
       " 'Berger',\n",
       " 'technician',\n",
       " 'rubbed',\n",
       " 'Meyerbeer',\n",
       " 'threshhold',\n",
       " 'replies',\n",
       " 'government-controlled',\n",
       " 'Representative',\n",
       " 'hood',\n",
       " 'structured',\n",
       " 'painfully',\n",
       " 'streetcars',\n",
       " 'dress',\n",
       " 'secondary',\n",
       " 'satisfies',\n",
       " 'Technician',\n",
       " 'spoilage',\n",
       " 'Beckman',\n",
       " 'underlined',\n",
       " 'Ireland',\n",
       " 'contemptuous',\n",
       " '1310',\n",
       " 'Anti-trust',\n",
       " 'familial',\n",
       " 'constructively',\n",
       " 'bloods',\n",
       " 'tamp',\n",
       " 'wondrously',\n",
       " '7082',\n",
       " 'ACS',\n",
       " 'engagingly',\n",
       " 'forwarded',\n",
       " 'grace',\n",
       " 'therefore',\n",
       " 'Merc',\n",
       " 'Abernathys',\n",
       " 'halfway',\n",
       " 'anachronistically',\n",
       " 'Museum',\n",
       " 'apprehended',\n",
       " 'shackled',\n",
       " 'bedfast',\n",
       " 'unhealthy',\n",
       " 'abandoned',\n",
       " 'interregnum',\n",
       " 'showered',\n",
       " 'distal',\n",
       " 'viewpoints',\n",
       " 'transferred',\n",
       " 'relives',\n",
       " 'connivance',\n",
       " 'ritual',\n",
       " 'maximizing',\n",
       " 'dull-gray',\n",
       " 'Laboratories',\n",
       " 'immutable',\n",
       " 'brothel',\n",
       " 'insignificance',\n",
       " '1957',\n",
       " 'cultures',\n",
       " 'consulting',\n",
       " 'Instead',\n",
       " 'positioned',\n",
       " 'Mio',\n",
       " 'top-quality',\n",
       " 'Hobart',\n",
       " 'moldboard',\n",
       " 'bovine',\n",
       " 'reportage',\n",
       " 'timely',\n",
       " 'formal',\n",
       " 'streamed',\n",
       " 'Lumber',\n",
       " 'wanta',\n",
       " 'plasterer',\n",
       " 'tellers',\n",
       " 'timbers',\n",
       " 'Lublin',\n",
       " 'Carre',\n",
       " '1986',\n",
       " '169',\n",
       " 'riches',\n",
       " 'hopelessness',\n",
       " '6934',\n",
       " 'morticians',\n",
       " 'Educational',\n",
       " 'Sad',\n",
       " 'Medecine',\n",
       " 'protects',\n",
       " 'syntax',\n",
       " 'Savage',\n",
       " \"O'Dwyers\",\n",
       " 'Orioles',\n",
       " 'Bast',\n",
       " 'skirt',\n",
       " 'Connall',\n",
       " 'Coupling',\n",
       " 'Nation',\n",
       " 'Holes',\n",
       " 'suffused',\n",
       " 'clocks',\n",
       " 'lower-class',\n",
       " 'relish',\n",
       " 'Sitting',\n",
       " 'Enos',\n",
       " 'pawing',\n",
       " 'berth',\n",
       " 'cancels',\n",
       " 'out-of-the-way',\n",
       " 'manservant',\n",
       " 'serve',\n",
       " 'Village',\n",
       " 'assimilation',\n",
       " 'under-par',\n",
       " 'shuffle',\n",
       " 'portentous',\n",
       " 'overcoat',\n",
       " 'tomb',\n",
       " 'weightlessness',\n",
       " 'surprising',\n",
       " 'gentility',\n",
       " 'Antoine',\n",
       " 'sup',\n",
       " 'Pike',\n",
       " 'election',\n",
       " 'mill',\n",
       " 'tribesmen',\n",
       " 'petitioned',\n",
       " 'economical',\n",
       " 'J.',\n",
       " 'theses',\n",
       " 'reason',\n",
       " 'Moroccan',\n",
       " 'existing',\n",
       " 'horizontal',\n",
       " 'Nicodemus',\n",
       " 'China',\n",
       " 'petit',\n",
       " 'alerting',\n",
       " 'patrons',\n",
       " 'Queried',\n",
       " 'candidly',\n",
       " 'presumption',\n",
       " 'smolderingly',\n",
       " 'Rawson',\n",
       " 'fainted',\n",
       " 'seashore',\n",
       " 'cost-plus',\n",
       " 'hypnosis',\n",
       " 'Smokies',\n",
       " 'fiberglas',\n",
       " 'decried',\n",
       " 'Chippendale',\n",
       " 'pivoting',\n",
       " 'nagged',\n",
       " 'clairvoyant',\n",
       " 'Centralia',\n",
       " 'reined',\n",
       " 'corn',\n",
       " 'slenderer',\n",
       " 'Enver',\n",
       " 'fiendish',\n",
       " 'nubbins',\n",
       " 'proposal',\n",
       " 'ebbing',\n",
       " 'officiating',\n",
       " 'Greek',\n",
       " 'unarmed',\n",
       " 'scoured',\n",
       " 'heretofore',\n",
       " 'sanatorium',\n",
       " 'wastage',\n",
       " 'Investment',\n",
       " 'text',\n",
       " 'viewer',\n",
       " 'Morningstar',\n",
       " 'major',\n",
       " 'Taste',\n",
       " 'Ahrens',\n",
       " 'transposition',\n",
       " 'anarchical',\n",
       " 'Heckman',\n",
       " 'suey',\n",
       " 'Trouble',\n",
       " 'paleocortical',\n",
       " 'anatomy',\n",
       " 'hardships',\n",
       " 'adopted',\n",
       " 'lenient',\n",
       " 'Inspector',\n",
       " 'fearful',\n",
       " 'Carmody',\n",
       " 'pave',\n",
       " 'blob',\n",
       " 'characteristics',\n",
       " 'choreographed',\n",
       " 'chastisement',\n",
       " 'fluid',\n",
       " 'blinked',\n",
       " 'carelessly',\n",
       " 'boldness',\n",
       " 'canning',\n",
       " 'Schultz',\n",
       " 'outraged',\n",
       " 'knock-down',\n",
       " 'dark-brown',\n",
       " 'Prestige',\n",
       " 'Students',\n",
       " 'face-to-face',\n",
       " 'Bromfield',\n",
       " 'Cup',\n",
       " 'night',\n",
       " 'teas',\n",
       " 'bongo',\n",
       " 'Westfield',\n",
       " 'Hanging',\n",
       " 'title-holder',\n",
       " 'lever-action',\n",
       " 'Impressions',\n",
       " 'approximations',\n",
       " 'Abreaction',\n",
       " 'sauces',\n",
       " 'rpm',\n",
       " 'Tell',\n",
       " 'safeguard',\n",
       " 'dropped',\n",
       " 'blooms',\n",
       " 'belched',\n",
       " 'set-up',\n",
       " 'bearings',\n",
       " 'furnished',\n",
       " 'transversely',\n",
       " 'agitating',\n",
       " 'worse',\n",
       " 'foregoing',\n",
       " 'Heitschmidt',\n",
       " 'particle',\n",
       " 'Forty',\n",
       " 'escalation',\n",
       " 'mopping',\n",
       " 'blasphemous',\n",
       " 'tongue-thrusting',\n",
       " 'Bleckley',\n",
       " '173',\n",
       " 'flourishing',\n",
       " 'Bubenik',\n",
       " 'emblematic',\n",
       " 'prognosis',\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28528])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana = torch.LongTensor([word2index['banana']])\n",
    "banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2721,  0.3790]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana_embed_c = model.embedding_center(banana)\n",
    "banana_embed_o = model.embedding_outside(banana)\n",
    "banana_embed   = (banana_embed_c + banana_embed_o) / 2\n",
    "banana_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6656,  1.0426]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana_embed_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "        \n",
    "    word = torch.LongTensor([word2index[word]])\n",
    "    \n",
    "    embed_c = model.embedding_center(word)\n",
    "    embed_o = model.embedding_outside(word)\n",
    "    embed   = (embed_c + embed_o) / 2\n",
    "    \n",
    "    return embed[0][0].item(), embed[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4043092429637909, -0.9147692322731018)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed('fruit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03199125453829765, 0.4378172755241394)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.28775718808174133, 0.9247953295707703)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.2720649540424347, 0.37896615266799927)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed('banana')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.2720649540424347, 0.37896615266799927)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana = get_embed('banana')\n",
    "banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4043092429637909, -0.9147692322731018)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit = get_embed('fruit')\n",
    "fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.4432022273540497, -0.4926328659057617)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk = get_embed('<UNK>')\n",
    "unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06611138825353269"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(banana) @ np.array(unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.21385672431033478\n",
      "-0.9787579100776338\n"
     ]
    }
   ],
   "source": [
    "#more formally is to divide by its norm\n",
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_a = np.linalg.norm(A)\n",
    "    norm_b = np.linalg.norm(B)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity\n",
    "\n",
    "print(cosine_similarity(np.array(banana), np.array(unk)))\n",
    "print(cosine_similarity(np.array(banana), np.array(fruit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
