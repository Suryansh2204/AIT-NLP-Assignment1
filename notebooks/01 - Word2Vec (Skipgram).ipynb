{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec (Skipgram )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package semcor to\n",
      "[nltk_data]     C:\\Users\\surya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\surya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import semcor, stopwords\n",
    "\n",
    "nltk.download('semcor')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA GeForce RTX 2080 with Max-Q Design')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor.words()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. tokenization\n",
    "corpus = semcor.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. numeralization\n",
    "#find unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "#assign unique integer\n",
    "vocabs = list(set(flatten(corpus))) #all the words we have in the system - <UNK>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create handy mapping between integer and word\n",
    "word2index = {v:idx for idx, v in enumerate(vocabs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = len(vocabs) - 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pairs of center word, and outside word\n",
    "\n",
    "def random_batch(batch_size, corpus, window_size=2):\n",
    "\n",
    "    skipgrams = []\n",
    "\n",
    "    #loop each corpus\n",
    "    for doc in corpus:\n",
    "        #look from the 2nd word until second last word\n",
    "        for i in range(window_size, len(doc)-window_size):\n",
    "            #center word\n",
    "            center = word2index[doc[i]]\n",
    "            \n",
    "            #outside words (window size on both sides of the center word)\n",
    "            outside_start =  i - window_size\n",
    "            outside_end =  i + window_size + 1 # +1 because the end index is exclusive\n",
    "\n",
    "            # outside = []\n",
    "            # loop through the outside words, append to the list 'outside'\n",
    "            for j in range(outside_start, outside_end):\n",
    "                if i != j:  # Skip the center word\n",
    "                    outside= word2index[doc[j]]\n",
    "                    skipgrams.append([center, outside])\n",
    "\n",
    "                \n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "    \n",
    "    inputs, labels = [], []\n",
    "    for index in random_index:\n",
    "        inputs.append([skipgrams[index][0]])\n",
    "        labels.append([skipgrams[index][1]])\n",
    "        \n",
    "    return np.array(inputs), np.array(labels)\n",
    "            \n",
    "x, y = random_batch(2, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33356],\n",
       "       [19595]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape  #batch_size 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\\log P(w_{t+j} | w_t; \\theta)$$\n",
    "\n",
    "where $P(w_{t+j} | w_t; \\theta) = $\n",
    "\n",
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$\n",
    "\n",
    "where $o$ is the outside words and $c$ is the center word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center)  #(batch_size, 1, emb_size)\n",
    "        outside_embedding    = self.embedding_center(outside) #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embedding = self.embedding_center(all_vocabs) #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        #batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1) \n",
    "\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        #batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size) \n",
    "        \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)  #(batch_size, 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))  #scalar\n",
    "        \n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ..., 43414, 43415, 43416],\n",
       "        [    0,     1,     2,  ..., 43414, 43415, 43416]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare all vocabs\n",
    "\n",
    "batch_size = 2\n",
    "voc_size   = len(vocabs)\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skipgram(\n",
       "  (embedding_center): Embedding(43417, 2)\n",
       "  (embedding_outside): Embedding(43417, 2)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Skipgram(voc_size, 2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.LongTensor(x)\n",
    "label_tensor = torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(input_tensor, label_tensor, all_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8671, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "emb_size   = 2\n",
    "model      = Skipgram(voc_size, emb_size)\n",
    "optimizer  = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def log_epoch(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss: 10.053512 | Time: 2m 48s\n",
      "Epoch: 20 | Loss: 12.454390 | Time: 5m 37s\n",
      "Epoch: 30 | Loss: 10.791930 | Time: 8m 27s\n",
      "Epoch: 40 | Loss: 11.176625 | Time: 11m 19s\n",
      "Epoch: 50 | Loss: 11.075615 | Time: 14m 10s\n",
      "Epoch: 60 | Loss: 12.948166 | Time: 17m 1s\n",
      "Epoch: 70 | Loss: 11.798746 | Time: 19m 51s\n",
      "Epoch: 80 | Loss: 11.828511 | Time: 22m 46s\n",
      "Epoch: 90 | Loss: 12.305994 | Time: 25m 40s\n",
      "Epoch: 100 | Loss: 12.651293 | Time: 28m 31s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "num_epochs = 100\n",
    "window_size = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus, window_size)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    #predict\n",
    "    loss = model(input_tensor, label_tensor, all_vocabs)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    # Log epoch time\n",
    "    epoch_mins, epoch_secs = log_epoch(start, time.time())\n",
    "    \n",
    "    #print the loss\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | Loss: {loss:.6f} | Time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot the embeddings\n",
    "\n",
    "Is fruit really near to banana?\n",
    "Is fruit really far from cat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Harold',\n",
       " 'tongs',\n",
       " 'wanted',\n",
       " 'negatively',\n",
       " 'anchored',\n",
       " 'Miss',\n",
       " 'completing',\n",
       " 'buoys',\n",
       " 'receptionist',\n",
       " 'imaging',\n",
       " 'Drum',\n",
       " 'Doctors',\n",
       " 'enlightened',\n",
       " 'hideaway',\n",
       " 'blandly',\n",
       " 'imperative',\n",
       " 'lettin',\n",
       " 'creek-filled',\n",
       " 'Payments',\n",
       " 'fawn',\n",
       " 'Corder',\n",
       " 'canister',\n",
       " 'crude',\n",
       " 'Aggie',\n",
       " 'Entwhistle',\n",
       " 'pale',\n",
       " 'unlined',\n",
       " 'pestering',\n",
       " 'tabula',\n",
       " 'Depression',\n",
       " 'f',\n",
       " 'tobacco',\n",
       " 'Schelling',\n",
       " 'haphazard',\n",
       " 'dived',\n",
       " 'Course',\n",
       " 'awful',\n",
       " 'Manderscheid',\n",
       " 'ornamented',\n",
       " '920',\n",
       " 'creep',\n",
       " 'Manufacturers',\n",
       " '8293',\n",
       " 'scopes',\n",
       " 'out-moded',\n",
       " 'deposits',\n",
       " 'lettering',\n",
       " 'posture',\n",
       " 'Result',\n",
       " 'Trackdown',\n",
       " 'equal',\n",
       " 'Culture',\n",
       " 'servitors',\n",
       " 'Carmer',\n",
       " 'armor',\n",
       " 'banners',\n",
       " 'consolidation',\n",
       " 'rattler',\n",
       " 'Waggin',\n",
       " '2.1',\n",
       " 'Bester',\n",
       " 'calmed',\n",
       " 'Brevard',\n",
       " 'peasant',\n",
       " 'exuberant',\n",
       " 'Isolde',\n",
       " 'falsehood',\n",
       " 'dispute',\n",
       " 'medical',\n",
       " 'encounter',\n",
       " 'stimulus',\n",
       " 'steepest',\n",
       " 'virtuosi',\n",
       " 'tunneled',\n",
       " 'nudes',\n",
       " 'Korra',\n",
       " 'Sid',\n",
       " 'trades',\n",
       " 'Multiply',\n",
       " 'no-hit',\n",
       " 'speak-easy',\n",
       " 'thanks',\n",
       " 'stubbornly',\n",
       " 'Manning',\n",
       " 'Burnt',\n",
       " 'Serieuses',\n",
       " 'sample',\n",
       " 'Inna',\n",
       " 'seepage',\n",
       " 'hand-me-down',\n",
       " '14',\n",
       " 'modernized',\n",
       " 'unimpeachably',\n",
       " 'ago',\n",
       " 'Myth',\n",
       " 'filler',\n",
       " 'Linking',\n",
       " 'Corinthians',\n",
       " 'interfaces',\n",
       " 'casino',\n",
       " 'Littleton',\n",
       " 'wrappin',\n",
       " 'burnt',\n",
       " 'premium',\n",
       " 'dipole',\n",
       " 'Midi',\n",
       " 'Skeletal',\n",
       " 'oft-repeated',\n",
       " 'Principals',\n",
       " 'Arabian',\n",
       " 'Purse',\n",
       " 'bills',\n",
       " 'ballerina',\n",
       " 'circumspect',\n",
       " '3500000',\n",
       " 'curiously',\n",
       " 'skirting',\n",
       " 'consciences',\n",
       " 'Dilworth',\n",
       " 'unobtainable',\n",
       " 'wall-switch',\n",
       " 'disputes',\n",
       " 'consent',\n",
       " 'moderates',\n",
       " 'underbelly',\n",
       " 'Pond',\n",
       " 'awaits',\n",
       " 'group',\n",
       " 'alphabet',\n",
       " 'Bisque',\n",
       " 'disappears',\n",
       " 'parks',\n",
       " 'awnings',\n",
       " 'proteolysis',\n",
       " 'Drafted',\n",
       " 'repulsion',\n",
       " 'flange',\n",
       " 'happenings',\n",
       " 'graves',\n",
       " 'payoff',\n",
       " 'Barcus',\n",
       " 'Blois',\n",
       " 'sheets',\n",
       " 'fortunes',\n",
       " 'breaths',\n",
       " 'cloddishness',\n",
       " 'between',\n",
       " 'Evening',\n",
       " 'sunt',\n",
       " 'generously',\n",
       " 'expressiveness',\n",
       " 'revenuers',\n",
       " 'Provisional',\n",
       " 'Askington',\n",
       " 'oversimplification',\n",
       " 'predicament',\n",
       " 'parolees',\n",
       " 'clubs',\n",
       " 'picture-palace',\n",
       " 'repaired',\n",
       " 'stamping',\n",
       " 'curbing',\n",
       " '31730',\n",
       " 'neuronal',\n",
       " 'fluoresces',\n",
       " 'pointing',\n",
       " 'elementary',\n",
       " 'Frito',\n",
       " 'Bodily',\n",
       " 'singers',\n",
       " '260',\n",
       " 'pummeled',\n",
       " 'ironies',\n",
       " 'Willett',\n",
       " 'Trikojus',\n",
       " 'Startled',\n",
       " 'tapping',\n",
       " 'despise',\n",
       " 'Beallsville',\n",
       " 'swindled',\n",
       " 'Florican-Inverness',\n",
       " 'Thank',\n",
       " 'Pump',\n",
       " 'lavishly',\n",
       " 'treaties',\n",
       " 'Twenty-six',\n",
       " 'Greylag',\n",
       " 'gruff',\n",
       " 'Blumenthal',\n",
       " 'Jay',\n",
       " 'Indigestion',\n",
       " 'steoreotyped',\n",
       " 'Teeth',\n",
       " 'comico-romantico',\n",
       " 'calibrating',\n",
       " '9b',\n",
       " 'nudging',\n",
       " 'impartial',\n",
       " 'Durrell',\n",
       " 'waterside',\n",
       " 'eatings',\n",
       " 'salesgirl',\n",
       " '3:30',\n",
       " 'worrying',\n",
       " 'Incorrect',\n",
       " 'Turbulent',\n",
       " 'Abatuno',\n",
       " 'Preparations',\n",
       " 'greening',\n",
       " 'notice',\n",
       " 'overheard',\n",
       " 'Spinley',\n",
       " 'Baringer',\n",
       " 'copiously',\n",
       " 'anastomotic',\n",
       " 'cowling',\n",
       " '434',\n",
       " 'air-conditioning',\n",
       " 'Ordinarily',\n",
       " 'home-comings',\n",
       " 'Farm',\n",
       " 'Benson',\n",
       " 'traveller',\n",
       " 'evaporation',\n",
       " 'Nod',\n",
       " 'cardiomegaly',\n",
       " 'postmark',\n",
       " 'taxpayers',\n",
       " 'penalty',\n",
       " 'interval',\n",
       " 'springboard',\n",
       " 'banner',\n",
       " 'theaf',\n",
       " 'Wet',\n",
       " 'Malmud',\n",
       " 'sifted',\n",
       " 'catsup',\n",
       " 'overland',\n",
       " 'dried-out',\n",
       " 'monotony',\n",
       " 'Rh',\n",
       " 'Transcendentalists',\n",
       " 'airline',\n",
       " 'quench',\n",
       " 'Albanian',\n",
       " 'adolescents',\n",
       " 'assertion',\n",
       " 'Harnick',\n",
       " 'stiffening',\n",
       " 'memorize',\n",
       " 'Retired',\n",
       " 'contestants',\n",
       " 'Apartments',\n",
       " 'naturalness',\n",
       " 'intelligent',\n",
       " 'affect',\n",
       " 'granules',\n",
       " 'sixth',\n",
       " 'privilege',\n",
       " 'campers',\n",
       " 'Congdon',\n",
       " 'tungsten',\n",
       " 'Sultan',\n",
       " 'airplanes',\n",
       " 'Swiss',\n",
       " 'justifiably',\n",
       " 'amplitude',\n",
       " 'scream',\n",
       " 'Mainland',\n",
       " 'catecholamines',\n",
       " 'aspired',\n",
       " 'dust-settling',\n",
       " 'repeater',\n",
       " 'Events',\n",
       " 'mausoleum',\n",
       " 'gee',\n",
       " 'mayst',\n",
       " 'defining',\n",
       " 'sad',\n",
       " 'interwoven',\n",
       " 'Letitia',\n",
       " 'sculptures',\n",
       " 'Sloan',\n",
       " 'ice',\n",
       " 'watt',\n",
       " 'womb-to-tomb',\n",
       " 'glasses',\n",
       " 'DiMaggio',\n",
       " 'Protestants',\n",
       " 'asserts',\n",
       " 'Billings',\n",
       " 'outboard',\n",
       " 'nineteenth-century',\n",
       " 'adjuncts',\n",
       " 'institutionalized',\n",
       " 'Visiting',\n",
       " 'redeeming',\n",
       " 'cholelithiasis',\n",
       " 'Wingman',\n",
       " 'Trio',\n",
       " 'ink',\n",
       " 'aisle',\n",
       " 'sanatorium',\n",
       " 'scratched',\n",
       " 'abstractionism',\n",
       " 'robed',\n",
       " 'Briefer',\n",
       " 'Imperial',\n",
       " 'Tiles',\n",
       " 'furnishings',\n",
       " 'rarity',\n",
       " 'spearhead',\n",
       " 'weekly',\n",
       " 'original',\n",
       " 'silicone',\n",
       " 'violinist',\n",
       " 'galleys',\n",
       " 'rubicund',\n",
       " 'resistor',\n",
       " 'tango',\n",
       " '639',\n",
       " 'bullying',\n",
       " 'taxes',\n",
       " 'cleat',\n",
       " 'persuading',\n",
       " 'Somers',\n",
       " 'formalized',\n",
       " 'Cleaner',\n",
       " 'auspiciously',\n",
       " 'Century-Fox',\n",
       " 'folk-lore',\n",
       " 'erratic',\n",
       " 'desirable',\n",
       " 'Royce',\n",
       " 'Nov',\n",
       " 'Sheer',\n",
       " 'Olney',\n",
       " 'diagnosticians',\n",
       " 'Accept',\n",
       " 'middle-age',\n",
       " 'maturational',\n",
       " 'Pesce',\n",
       " 'announcers',\n",
       " 'Leaning',\n",
       " 'evenings',\n",
       " 'salons',\n",
       " '16.38',\n",
       " 'Korea',\n",
       " 'rehearsed',\n",
       " 'rougher',\n",
       " 'leave',\n",
       " 'communicator',\n",
       " 'Bathtubs',\n",
       " 'Sacre',\n",
       " 'tattered',\n",
       " 'Dylan',\n",
       " 'hops',\n",
       " 'tenant',\n",
       " 'external',\n",
       " 'Star-Spangled',\n",
       " '1.25',\n",
       " '443',\n",
       " 'bellowed',\n",
       " 'dispenser',\n",
       " 'finances',\n",
       " 'Parapsychology',\n",
       " 'scientific',\n",
       " 'pre-Han',\n",
       " 'Silas',\n",
       " '2454',\n",
       " 'Cezannes',\n",
       " 'happier',\n",
       " 'Northwestern',\n",
       " 'corporations',\n",
       " 'de-iodinated',\n",
       " 'sniggered',\n",
       " \"'49\",\n",
       " 'Roads',\n",
       " 'hitting',\n",
       " 'burnings',\n",
       " 'speak',\n",
       " 'Enthusiastically',\n",
       " 'Different',\n",
       " 'phenomenon',\n",
       " 'Edison',\n",
       " 'flashing',\n",
       " 'arrogate',\n",
       " 'transactions',\n",
       " 'occasionally',\n",
       " 'hedonism',\n",
       " 'brand',\n",
       " 'crippled',\n",
       " 'zooming',\n",
       " 'Tony',\n",
       " 'Blanche',\n",
       " 'glue',\n",
       " 'Niagara',\n",
       " 'monologue',\n",
       " 'twentieth-century',\n",
       " 'gaining',\n",
       " 'homer',\n",
       " 'gentleness',\n",
       " 'romancers',\n",
       " 'proxy',\n",
       " 'Share',\n",
       " 'Senesac',\n",
       " 'Roylott',\n",
       " 'aggressions',\n",
       " 'artisan',\n",
       " 'Collett',\n",
       " 'obsolete',\n",
       " 'handsomer',\n",
       " 'editions',\n",
       " 'haunt',\n",
       " 'Fifty-three',\n",
       " 'topsoil',\n",
       " 'assert',\n",
       " 'individually',\n",
       " 'Cygne',\n",
       " 'deltoids',\n",
       " 'mobilize',\n",
       " 'unspecified',\n",
       " 'Colcord',\n",
       " 'handed',\n",
       " 'devoured',\n",
       " 'denuded',\n",
       " 'ejaculated',\n",
       " 'drafters',\n",
       " 'repent',\n",
       " 'reproduces',\n",
       " 'sedate',\n",
       " 'Adventures',\n",
       " 'Witold',\n",
       " 'feller',\n",
       " 'later',\n",
       " 'undeniably',\n",
       " 'hardener',\n",
       " 'inefficiency',\n",
       " 'calorie-heavy',\n",
       " 'Julie',\n",
       " 'futility',\n",
       " 'knowledge',\n",
       " 'half-brother',\n",
       " 'tsunami',\n",
       " 'trenchant',\n",
       " 'steadier',\n",
       " 'ajar',\n",
       " 'yesterday',\n",
       " 'cps',\n",
       " 'dwellings',\n",
       " 'Potter',\n",
       " 'single-handedly',\n",
       " 'Melville',\n",
       " 'reciprocal',\n",
       " 'frost-bitten',\n",
       " 'estrangement',\n",
       " 'sailor',\n",
       " 'pastilles',\n",
       " 'Conceding',\n",
       " 'overindulged',\n",
       " 'chord',\n",
       " 'ornraier',\n",
       " 'resemblance',\n",
       " 'fur',\n",
       " 'theirs',\n",
       " 'Barnumville',\n",
       " 'Trim',\n",
       " 'stable',\n",
       " 'Elder',\n",
       " 'distinguish',\n",
       " 'panorama',\n",
       " '57500',\n",
       " 'Minerals',\n",
       " 'imagination',\n",
       " 'saint',\n",
       " 'poorly',\n",
       " 'cometary',\n",
       " 'disregarded',\n",
       " 'Falcon',\n",
       " 'Moluccas',\n",
       " 'Apalachicola',\n",
       " 'pacing',\n",
       " 'clearer',\n",
       " 'matches',\n",
       " 'Expressway',\n",
       " 'talented',\n",
       " 'overlying',\n",
       " 'sometimes-necessary',\n",
       " 'anaerobic',\n",
       " 'Heckman',\n",
       " 'curricular',\n",
       " 'handkerchiefs',\n",
       " 'bureaucrat',\n",
       " 'bludgeon',\n",
       " 'Homicide',\n",
       " 'folk',\n",
       " 'faucet',\n",
       " 'waterline',\n",
       " 'destroying',\n",
       " 'freight',\n",
       " 'medico',\n",
       " 'Representatives',\n",
       " 'Menilmontant',\n",
       " 'plants',\n",
       " 'brisk',\n",
       " 'presumes',\n",
       " 'Maltese',\n",
       " 'manipulated',\n",
       " '24',\n",
       " 'Homemade',\n",
       " 'Sun-Times',\n",
       " 'Resolve',\n",
       " 'pistoleers',\n",
       " 'sho',\n",
       " 'poises',\n",
       " 'feedback',\n",
       " 'Months',\n",
       " 'hopscotch',\n",
       " 'examiner',\n",
       " 'lost',\n",
       " 'Astree',\n",
       " 'fragmentary',\n",
       " 'accompany',\n",
       " 'Melody',\n",
       " 'increased',\n",
       " 'blessed',\n",
       " 'mounts',\n",
       " 'His',\n",
       " 'dock',\n",
       " 'Bakhtiari',\n",
       " 'insets',\n",
       " 'corporis',\n",
       " 'perception',\n",
       " 'allotments',\n",
       " 'neckline',\n",
       " 'Omission',\n",
       " 'unambiguity',\n",
       " 'Screw',\n",
       " 'pondered',\n",
       " 'jubilantly',\n",
       " 'India',\n",
       " 'infuriated',\n",
       " 'nonviolent',\n",
       " 'Librarians',\n",
       " 'A-1',\n",
       " 'Twilight',\n",
       " 'demonstrate',\n",
       " 'critics',\n",
       " 'approvingly',\n",
       " 'overtaken',\n",
       " 'glacier',\n",
       " 'Comique',\n",
       " 'Marquis',\n",
       " '101',\n",
       " 'pest',\n",
       " 'well-played',\n",
       " 'allowed',\n",
       " 'weird',\n",
       " 'compensate',\n",
       " 'blitzes',\n",
       " 'cooky',\n",
       " 'five-and-a-half',\n",
       " 'ashen',\n",
       " 'collector',\n",
       " 'paleo',\n",
       " 'boomtown',\n",
       " 'Pettersson',\n",
       " 'ad',\n",
       " 'spilled',\n",
       " 'choreographic',\n",
       " 'Distributive',\n",
       " 'Lanesmanship',\n",
       " 'poncho',\n",
       " 'Beaverton',\n",
       " 'Len',\n",
       " 'Frozen',\n",
       " 'investigations',\n",
       " 'Ana',\n",
       " 'incurs',\n",
       " 'Hawkins',\n",
       " 'selecting',\n",
       " 'volcanic',\n",
       " 'fly-boy',\n",
       " 'much',\n",
       " '550',\n",
       " 'etiquette',\n",
       " 'appropriations',\n",
       " 'replanted',\n",
       " 'alerts',\n",
       " 'best-sellers',\n",
       " 'Mises',\n",
       " 'pertain',\n",
       " 'overshadowed',\n",
       " 'Launder-Ometer',\n",
       " 'Treatment',\n",
       " 'amateurishness',\n",
       " 'scholarships',\n",
       " 'blunt',\n",
       " 'Horse',\n",
       " 'dulls',\n",
       " 'classificatory',\n",
       " 'Keys',\n",
       " 'pint',\n",
       " 'leggy',\n",
       " 'canceling',\n",
       " 'face-saving',\n",
       " 'pleasure',\n",
       " 'honour',\n",
       " 'chansons',\n",
       " 'coaxial',\n",
       " 'Keng',\n",
       " 'Ziffren',\n",
       " 'hasher',\n",
       " 'concave',\n",
       " 'intra-stellar',\n",
       " 'agates',\n",
       " 'expanse',\n",
       " 'significant',\n",
       " 'bravado',\n",
       " 'wrap',\n",
       " 'Nineteen',\n",
       " 'one-man',\n",
       " 'dealt',\n",
       " 'badness',\n",
       " 'Eileen',\n",
       " 'modification',\n",
       " 'nondefeatist',\n",
       " 'Class',\n",
       " 'auf',\n",
       " 'implement',\n",
       " 'limiting',\n",
       " 'Henry',\n",
       " 'polka',\n",
       " 'prepare',\n",
       " 'Conferences',\n",
       " 'tenacity',\n",
       " 'incessantly',\n",
       " 'royalties',\n",
       " 'Again',\n",
       " 'meet',\n",
       " 'cast-iron',\n",
       " 'luminosity',\n",
       " 'dumbbells',\n",
       " 'paving',\n",
       " 'molest',\n",
       " 'Consort',\n",
       " 'los',\n",
       " 'white-stucco',\n",
       " 'Molinari',\n",
       " 'Thru',\n",
       " 'Hettie',\n",
       " 'Metronome',\n",
       " 'Ozarks',\n",
       " 'terribly',\n",
       " 'chieftain',\n",
       " 'Lorrain',\n",
       " 'Varner',\n",
       " 'Alberto',\n",
       " 'telegraphic',\n",
       " 'prospered',\n",
       " 'Schaack',\n",
       " 'Arkabutla',\n",
       " 'Hart',\n",
       " 'suicides',\n",
       " 'Organization',\n",
       " 'polarity',\n",
       " 'betrothal',\n",
       " 'big',\n",
       " '98',\n",
       " 'Mathematical',\n",
       " 'year-to-year',\n",
       " 'unseen',\n",
       " 'assessor',\n",
       " 'extremis',\n",
       " 'neurosis',\n",
       " 'music-making',\n",
       " 'ity',\n",
       " 'adjournment',\n",
       " 'fourteen',\n",
       " 'esteem',\n",
       " 'upholding',\n",
       " 'removing',\n",
       " 'des',\n",
       " 'misconstruction',\n",
       " 'Lambert',\n",
       " 'expressionist',\n",
       " '242',\n",
       " 'shoestring',\n",
       " 'Sylvania',\n",
       " 'demonstrated',\n",
       " 'hirelings',\n",
       " 'uninterruptedly',\n",
       " 'Ethel',\n",
       " 'pledged',\n",
       " 'uncommon',\n",
       " 'flashback',\n",
       " 'skin',\n",
       " 'drifting',\n",
       " 'municipal',\n",
       " 'bequest',\n",
       " 'clerk',\n",
       " 'Ltd.',\n",
       " 'serial',\n",
       " 'slowly',\n",
       " 'jejunum',\n",
       " 'third-dimensional',\n",
       " 'focusing',\n",
       " 'brigadier',\n",
       " 'acquires',\n",
       " 'strips',\n",
       " \"can't\",\n",
       " '1850',\n",
       " 'Keizer',\n",
       " 'concordant',\n",
       " 'believer',\n",
       " 'Emptied',\n",
       " 'housewives',\n",
       " 'sparkled',\n",
       " 'clumsy',\n",
       " 'Self-criticism',\n",
       " 'endeared',\n",
       " 'nimbler',\n",
       " 'true-false',\n",
       " 'Sanger-Harris',\n",
       " 'saints',\n",
       " 'Nostalgia',\n",
       " 'gimbaled',\n",
       " 'lubra',\n",
       " 'ensembles',\n",
       " 'hunted',\n",
       " 'Campaigne',\n",
       " 'Charming',\n",
       " 'teachers',\n",
       " 'encampment',\n",
       " 'phonies',\n",
       " 'soviet',\n",
       " 'Galina',\n",
       " 'enrich',\n",
       " 'Magnums',\n",
       " 'wishful',\n",
       " 'Think',\n",
       " 'parody',\n",
       " 'desires',\n",
       " 'frills',\n",
       " 'demoralization',\n",
       " 'Kok',\n",
       " 'albumin',\n",
       " 'tuneful',\n",
       " 'mystery',\n",
       " 'instrument',\n",
       " 'Congratulations',\n",
       " 'drawing',\n",
       " 'Corsi',\n",
       " 'Superstition',\n",
       " 'leak',\n",
       " 'clemency',\n",
       " 'pretended',\n",
       " 'sequential',\n",
       " 'Participating',\n",
       " 'Initial',\n",
       " 'rustle',\n",
       " 'psychopath',\n",
       " 'Cecilia',\n",
       " 'failing',\n",
       " 'attorneys',\n",
       " 'geologist',\n",
       " 'quieter',\n",
       " 'innumerable',\n",
       " 'advance',\n",
       " 'bounty',\n",
       " 'protocol',\n",
       " 'lawmen',\n",
       " 'rollicking',\n",
       " 'expel',\n",
       " 'unwarranted',\n",
       " 'charisma',\n",
       " 'currents',\n",
       " 'Galveston-Port',\n",
       " 'Radical',\n",
       " '1800',\n",
       " 'inadvertently',\n",
       " 'newcomers',\n",
       " 'straightway',\n",
       " 'Grossman',\n",
       " 'intestines',\n",
       " 'Hillyer',\n",
       " 'Taiwan',\n",
       " 'paid-for',\n",
       " 'smolders',\n",
       " 'Fisher',\n",
       " 'Interesting',\n",
       " 'tilts',\n",
       " 'staggering',\n",
       " 'idea',\n",
       " 'heading',\n",
       " 'elaborate',\n",
       " 'tubs',\n",
       " 'defense',\n",
       " 'Conservatory',\n",
       " 'auxiliaries',\n",
       " 'Follow',\n",
       " 'offset',\n",
       " 'miner',\n",
       " 'Visitation',\n",
       " 'contests',\n",
       " 'infections',\n",
       " 'Shari',\n",
       " 'confidential',\n",
       " 'lappets',\n",
       " 'leaden',\n",
       " 'Thant',\n",
       " 'caste',\n",
       " 'gobbled',\n",
       " 'bookcases',\n",
       " 'voted',\n",
       " '126000',\n",
       " 'ate',\n",
       " 'long-sleeved',\n",
       " 'Doctor',\n",
       " 'Stamford',\n",
       " 'prettiest',\n",
       " 'ex-marine',\n",
       " 'sub-freezing',\n",
       " 'Zeising',\n",
       " 'protrusion',\n",
       " 'Turner',\n",
       " 'Life',\n",
       " 'silvery',\n",
       " 'sandy',\n",
       " 'Gallant',\n",
       " 'positively',\n",
       " 'bosom',\n",
       " 'Genevieve',\n",
       " 'Lisle',\n",
       " 'Jungle',\n",
       " 'doubt',\n",
       " 'boroughs',\n",
       " 'Jonquieres',\n",
       " 'Odom',\n",
       " 'thermodynamics',\n",
       " 'hemispherical',\n",
       " 'Bangs',\n",
       " 'Haase',\n",
       " 'Mattathias',\n",
       " 'splintery',\n",
       " 'sentimentalize',\n",
       " 'perceptual',\n",
       " 'intactible',\n",
       " 'bygone',\n",
       " 'Telefunken',\n",
       " 'advertising',\n",
       " 'faltered',\n",
       " 'Integration',\n",
       " 'disease',\n",
       " 'Laws',\n",
       " 'grape-arbor',\n",
       " 'Ethan',\n",
       " 'Jameson',\n",
       " 'Fiorello',\n",
       " 'streaked',\n",
       " 'three-month',\n",
       " 'fruitless',\n",
       " 'compulsory',\n",
       " 'Scout',\n",
       " 'Flat',\n",
       " 'convicts',\n",
       " 'proponent',\n",
       " 'Juanita',\n",
       " 'defended',\n",
       " 'Rak',\n",
       " 'locales',\n",
       " 'hissing',\n",
       " 'seekers',\n",
       " 'proved',\n",
       " 'interference',\n",
       " 'surviving',\n",
       " 'Frick',\n",
       " 'brimful',\n",
       " 'story',\n",
       " 'Cyclades',\n",
       " 'carborundum',\n",
       " 'oxytetracycline',\n",
       " 'Leesona',\n",
       " 'buried',\n",
       " 'booths',\n",
       " 'Acapulco',\n",
       " 'throes',\n",
       " 'make-up',\n",
       " 'journeys',\n",
       " 'attacker',\n",
       " 'particularistic',\n",
       " 'six-thirty',\n",
       " 'condenser',\n",
       " 'vous',\n",
       " 'Crusade',\n",
       " 'Supper',\n",
       " 'tunelessly',\n",
       " 'uni-directional',\n",
       " 'Nagle',\n",
       " 'tremor',\n",
       " 'Moscow-allied',\n",
       " 'better',\n",
       " 'Traverse',\n",
       " 'paling',\n",
       " 'fins',\n",
       " 'inflamed',\n",
       " 'juleps',\n",
       " 'full-scale',\n",
       " 'honors',\n",
       " 'handymen',\n",
       " 'fund-raising',\n",
       " 'corroborate',\n",
       " 'artillery',\n",
       " 'pie',\n",
       " 'reinforcements',\n",
       " 'Bodin',\n",
       " 'neo',\n",
       " 'zombies',\n",
       " 'backs',\n",
       " 'confessionals',\n",
       " 'Being',\n",
       " 'hereditary',\n",
       " 'drifted',\n",
       " 'Goodbody',\n",
       " 'Herridge',\n",
       " 'Junkers',\n",
       " 'bind',\n",
       " 'Irishman',\n",
       " 'Knecht',\n",
       " 'Displaying',\n",
       " 'Sidney',\n",
       " 'grassroots',\n",
       " 'fists',\n",
       " 'Amendments',\n",
       " 'inconsequential',\n",
       " 'shorter',\n",
       " 'bloodlust',\n",
       " 'Menderes',\n",
       " 'recruiter',\n",
       " 'charters',\n",
       " 'renaissance',\n",
       " 'consuming',\n",
       " 'thickness',\n",
       " 'weighing',\n",
       " 'Broke',\n",
       " 'shaven',\n",
       " 'Picon',\n",
       " 'subsidized',\n",
       " 'dashboard',\n",
       " 'corpus',\n",
       " 'Australian',\n",
       " 'femininity',\n",
       " 'indolent',\n",
       " 'vascular',\n",
       " 'Alexandria',\n",
       " 'Sabbath',\n",
       " 'Bulba',\n",
       " 'slave',\n",
       " 'Rawlins',\n",
       " 'robber',\n",
       " 'enzymatic',\n",
       " 'barbecues',\n",
       " '1119',\n",
       " 'exodus',\n",
       " 'gist',\n",
       " 'Frederick',\n",
       " 'detector',\n",
       " 'coosie',\n",
       " 'semipublic',\n",
       " 'nicknames',\n",
       " 'demented',\n",
       " 'Nagasaki',\n",
       " 'turn',\n",
       " 'electroshocks',\n",
       " 'averting',\n",
       " 'pacem',\n",
       " 'Estherson',\n",
       " 'lawful',\n",
       " 'navigating',\n",
       " 'disturbances',\n",
       " 'haired',\n",
       " 'unwrinkled',\n",
       " 'Dinner',\n",
       " 'SW',\n",
       " 'capitals',\n",
       " 'carport',\n",
       " 'raiser',\n",
       " 'admittance',\n",
       " '12000',\n",
       " 'rehearing',\n",
       " 'tombs',\n",
       " 'gage',\n",
       " 'dawn',\n",
       " 'jesting',\n",
       " 'McLendon-Ebony',\n",
       " 'Chien',\n",
       " 'homeowners',\n",
       " 'reasoning',\n",
       " 'Speer',\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([33519])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana = torch.LongTensor([word2index['banana']])\n",
    "banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9419, -0.0039]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana_embed_c = model.embedding_center(banana)\n",
    "banana_embed_o = model.embedding_outside(banana)\n",
    "banana_embed   = (banana_embed_c + banana_embed_o) / 2\n",
    "banana_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1480,  0.6964]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana_embed_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "        \n",
    "    word = torch.LongTensor([word2index[word]])\n",
    "    \n",
    "    embed_c = model.embedding_center(word)\n",
    "    embed_o = model.embedding_outside(word)\n",
    "    embed   = (embed_c + embed_o) / 2\n",
    "    \n",
    "    return embed[0][0].item(), embed[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4439716339111328, 0.6431342959403992)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed('fruit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.344612717628479, -0.31181710958480835)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7699832320213318, -0.04242171347141266)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.941861093044281, -0.003913760185241699)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed('banana')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.941861093044281, -0.003913760185241699)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana = get_embed('banana')\n",
    "banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4439716339111328, 0.6431342959403992)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit = get_embed('fruit')\n",
    "fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.053452007472515106, 0.762205183506012)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk = get_embed('<UNK>')\n",
    "unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05332745448366483"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(banana) @ np.array(unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.07410082563838405\n",
      "-0.5715211478067245\n"
     ]
    }
   ],
   "source": [
    "#more formally is to divide by its norm\n",
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_a = np.linalg.norm(A)\n",
    "    norm_b = np.linalg.norm(B)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity\n",
    "\n",
    "print(cosine_similarity(np.array(banana), np.array(unk)))\n",
    "print(cosine_similarity(np.array(banana), np.array(fruit)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'skipgram_model.pkl'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
