{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec (Skipgram )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading semcor: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import semcor\n",
    "\n",
    "nltk.download('semcor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'NVIDIA GeForce RTX 2080 with Max-Q Design')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor.words()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. tokenization\n",
    "corpus = semcor.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. numeralization\n",
    "#find unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "#assign unique integer\n",
    "vocabs = list(set(flatten(corpus))) #all the words we have in the system - <UNK>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create handy mapping between integer and word\n",
    "word2index = {v:idx for idx, v in enumerate(vocabs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = len(vocabs) - 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pairs of center word, and outside word\n",
    "\n",
    "def random_batch(batch_size, corpus, window_size=2):\n",
    "\n",
    "    skipgrams = []\n",
    "\n",
    "    #loop each corpus\n",
    "    for doc in corpus:\n",
    "        #look from the 2nd word until second last word\n",
    "        for i in range(window_size, len(doc)-window_size):\n",
    "            #center word\n",
    "            center = word2index[doc[i]]\n",
    "            \n",
    "            #outside words (window size on both sides of the center word)\n",
    "            outside_start =  i - window_size\n",
    "            outside_end =  i + window_size + 1 # +1 because the end index is exclusive\n",
    "\n",
    "            # outside = []\n",
    "            # loop through the outside words, append to the list 'outside'\n",
    "            for j in range(outside_start, outside_end):\n",
    "                if i != j:  # Skip the center word\n",
    "                    outside= word2index[doc[j]]\n",
    "                    skipgrams.append([center, outside])\n",
    "\n",
    "                \n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "    \n",
    "    inputs, labels = [], []\n",
    "    for index in random_index:\n",
    "        inputs.append([skipgrams[index][0]])\n",
    "        labels.append([skipgrams[index][1]])\n",
    "        \n",
    "    return np.array(inputs), np.array(labels)\n",
    "            \n",
    "x, y = random_batch(2, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27243],\n",
       "       [ 9809]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape  #batch_size 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\\log P(w_{t+j} | w_t; \\theta)$$\n",
    "\n",
    "where $P(w_{t+j} | w_t; \\theta) = $\n",
    "\n",
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$\n",
    "\n",
    "where $o$ is the outside words and $c$ is the center word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center)  #(batch_size, 1, emb_size)\n",
    "        outside_embedding    = self.embedding_center(outside) #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embedding = self.embedding_center(all_vocabs) #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        #batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1) \n",
    "\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        #batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size) \n",
    "        \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)  #(batch_size, 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))  #scalar\n",
    "        \n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ..., 43414, 43415, 43416],\n",
       "        [    0,     1,     2,  ..., 43414, 43415, 43416]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare all vocabs\n",
    "\n",
    "batch_size = 2\n",
    "voc_size   = len(vocabs)\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Skipgram(\n",
       "  (embedding_center): Embedding(43417, 2)\n",
       "  (embedding_outside): Embedding(43417, 2)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Skipgram(voc_size, 2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.LongTensor(x)\n",
    "label_tensor = torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(input_tensor, label_tensor, all_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.8140, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "emb_size   = 2\n",
    "model      = Skipgram(voc_size, emb_size)\n",
    "optimizer  = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_epoch(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss: 8.762987 | Time: 2m 58s\n",
      "Epoch: 20 | Loss: 10.320016 | Time: 5m 52s\n",
      "Epoch: 30 | Loss: 13.204978 | Time: 8m 43s\n",
      "Epoch: 40 | Loss: 11.407680 | Time: 11m 35s\n",
      "Epoch: 50 | Loss: 9.133223 | Time: 14m 25s\n",
      "Epoch: 60 | Loss: 12.455471 | Time: 17m 15s\n",
      "Epoch: 70 | Loss: 12.883157 | Time: 20m 3s\n",
      "Epoch: 80 | Loss: 11.338598 | Time: 22m 54s\n",
      "Epoch: 90 | Loss: 13.680546 | Time: 25m 44s\n",
      "Epoch: 100 | Loss: 13.209658 | Time: 28m 36s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "num_epochs = 100\n",
    "window_size = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus, window_size)\n",
    "    input_tensor = torch.LongTensor(input_batch)\n",
    "    label_tensor = torch.LongTensor(label_batch)\n",
    "    \n",
    "    #predict\n",
    "    loss = model(input_tensor, label_tensor, all_vocabs)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    # Log epoch time\n",
    "    epoch_mins, epoch_secs = log_epoch(start, time.time())\n",
    "    \n",
    "    #print the loss\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | Loss: {loss:.6f} | Time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot the embeddings\n",
    "\n",
    "Is fruit really near to banana?\n",
    "Is fruit really far from cat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Clerfayt',\n",
       " 'establishes',\n",
       " 'psychotherapists',\n",
       " 'marts',\n",
       " 'appreciating',\n",
       " 'operative',\n",
       " 'Directionality',\n",
       " 'jesting',\n",
       " 'thematic',\n",
       " 'illustrations',\n",
       " 'wastebasket',\n",
       " 'bobby-sox',\n",
       " 'devoured',\n",
       " 'destined',\n",
       " 'liberty',\n",
       " 'thirties',\n",
       " 'jumps',\n",
       " 'supremacy',\n",
       " 'babes',\n",
       " 'granting',\n",
       " 'condemnation',\n",
       " 'wolves',\n",
       " 'witches',\n",
       " 'Barker',\n",
       " 'mystic',\n",
       " 'Cuban-American',\n",
       " 'Aunt',\n",
       " 'affairs',\n",
       " 'reporters',\n",
       " 'waived',\n",
       " 'defaulted',\n",
       " 'mosquitoes',\n",
       " 'Strauss',\n",
       " 'chap.',\n",
       " 'hooting',\n",
       " 'diverging',\n",
       " 'Laboratory',\n",
       " 'pork-barrel',\n",
       " 'counter-attack',\n",
       " 'Cibula',\n",
       " 'dynastic',\n",
       " 'plantings',\n",
       " 'Thanks',\n",
       " 'Britain',\n",
       " 'Guinea',\n",
       " 'cyclist',\n",
       " 'admittedly',\n",
       " 'Frankfurt',\n",
       " 'ABO',\n",
       " 'Kroening',\n",
       " 'god-like',\n",
       " 'Carriages',\n",
       " 'Richardson',\n",
       " 'antagonize',\n",
       " 'outweighed',\n",
       " 'imitative',\n",
       " 'freehand',\n",
       " 'howling',\n",
       " 'Greyhound',\n",
       " 'reins',\n",
       " 'gist',\n",
       " 'heaped',\n",
       " 'Spectra',\n",
       " 'Torrio',\n",
       " 'warily',\n",
       " 'complications',\n",
       " 'dwell',\n",
       " 'profitably',\n",
       " 'sobbingly',\n",
       " 'stumbled',\n",
       " 'palsy',\n",
       " 'banner',\n",
       " 'contorted',\n",
       " 'unself-conscious',\n",
       " 'Brevard',\n",
       " 'sander',\n",
       " 'fabled',\n",
       " 'steeples',\n",
       " '85000',\n",
       " 'February',\n",
       " 'abolish',\n",
       " 'pickup',\n",
       " 'tappet',\n",
       " 'longshoremen',\n",
       " 'indoors',\n",
       " 'azimuth',\n",
       " 'unsurpassed',\n",
       " 'infectious',\n",
       " 'canisters',\n",
       " 'not-Involved',\n",
       " 'Lovelace',\n",
       " 'traveled',\n",
       " 'abject',\n",
       " 'ologies',\n",
       " 'tripods',\n",
       " '20000000000',\n",
       " 'planned',\n",
       " 'strengths',\n",
       " 'hand-screened',\n",
       " 'hypnotized',\n",
       " 'pulleys',\n",
       " 'machinist',\n",
       " 'malfeasant',\n",
       " 'collar-to-collar',\n",
       " 'Bldg.',\n",
       " 'Flies',\n",
       " 'bought',\n",
       " 'Iceland',\n",
       " 'programed',\n",
       " 'silenced',\n",
       " 'spaceship',\n",
       " 'reopened',\n",
       " 'scandalized',\n",
       " 'Tulsa',\n",
       " 'Hawk',\n",
       " 'Moses',\n",
       " 'sentences',\n",
       " 'Jobs',\n",
       " 'unavailing',\n",
       " 'burrowed',\n",
       " 'safeguard',\n",
       " 'busy',\n",
       " 'Charm',\n",
       " 'husbands',\n",
       " 'score',\n",
       " '.020',\n",
       " 'stimulus',\n",
       " 'clubs',\n",
       " 'Eldest',\n",
       " 'sloe',\n",
       " 'Representing',\n",
       " 'Lips',\n",
       " 'predict',\n",
       " 'answered',\n",
       " 'attached',\n",
       " 'systematically',\n",
       " 'hijacked',\n",
       " 'county-wide',\n",
       " 'mononuclear',\n",
       " '12.8',\n",
       " 'relationship-building',\n",
       " 'virulent',\n",
       " \"mother's\",\n",
       " 'Askington',\n",
       " 'desolations',\n",
       " 'Detective',\n",
       " 'undertaker',\n",
       " 'foamed',\n",
       " 'numerology',\n",
       " 'abdomen',\n",
       " 'region',\n",
       " 'Kern',\n",
       " 'Lilac',\n",
       " 'Freeport',\n",
       " '1891',\n",
       " 'initiating',\n",
       " 'slits',\n",
       " 'survives',\n",
       " 'studies',\n",
       " 'Indonesian',\n",
       " 'ahead',\n",
       " 'Meanings',\n",
       " 'Main',\n",
       " 'Centennial',\n",
       " 'rewt',\n",
       " 'Eckenfelder',\n",
       " 'wart',\n",
       " 'Sonenberg',\n",
       " 'powerfulness',\n",
       " 'baited',\n",
       " 'aspirin',\n",
       " 'af',\n",
       " 'Fired',\n",
       " 'squeaked',\n",
       " 'achievements',\n",
       " 'closely',\n",
       " 'Witnesses',\n",
       " 'Adcock',\n",
       " 'tyrosine',\n",
       " 'ambling',\n",
       " 'rests',\n",
       " 'harrowed',\n",
       " 'German',\n",
       " 'knockdown',\n",
       " 'gauged',\n",
       " 'footage',\n",
       " 'inhabiting',\n",
       " 'Harrington',\n",
       " 'Jules',\n",
       " 'coexistence',\n",
       " 'assay',\n",
       " 'bouanahsha',\n",
       " 'steamship',\n",
       " 'lawmen',\n",
       " 'Unconscionable',\n",
       " 'Tuberculosis',\n",
       " 'smallish',\n",
       " 'rose-of-Sharon',\n",
       " 'Savage',\n",
       " 'Sexual',\n",
       " 'warrants',\n",
       " 'Discourse',\n",
       " 'negligence',\n",
       " 'will',\n",
       " 'packages',\n",
       " '214',\n",
       " 'meme',\n",
       " 'Sanitary',\n",
       " 'Boron',\n",
       " 'clucks',\n",
       " 'suit',\n",
       " 'wifely',\n",
       " 'Provided',\n",
       " 'Lend',\n",
       " 'Leary',\n",
       " 'beetles',\n",
       " 'Pitch',\n",
       " 'ushered',\n",
       " 'martingale',\n",
       " 'Buckley',\n",
       " 'demineralization',\n",
       " 'Patients',\n",
       " 'grab',\n",
       " 'corded',\n",
       " 'shaken',\n",
       " 'beloved',\n",
       " 'constitutional',\n",
       " 'pulse-jet',\n",
       " '213',\n",
       " 'tugged',\n",
       " 'Lieut',\n",
       " 'Kikiyus',\n",
       " 'Klux',\n",
       " 'accompanies',\n",
       " 'Consonantal',\n",
       " 'glimpsed',\n",
       " 'Darlene',\n",
       " 'Marie',\n",
       " 'shaved',\n",
       " 'pre-Han',\n",
       " 'fatigue',\n",
       " 'admission',\n",
       " 'limbo',\n",
       " 'extracting',\n",
       " 'flexibility',\n",
       " 'Parents',\n",
       " 'depravity',\n",
       " 'constraint',\n",
       " 'Regions',\n",
       " 'compensated',\n",
       " 'bricklayers',\n",
       " 'strikes',\n",
       " 'Silvas',\n",
       " 'Ciardi',\n",
       " 'Byronism',\n",
       " 're-enforces',\n",
       " 'submits',\n",
       " 'defiantly',\n",
       " 'morning-glory',\n",
       " 'Electric',\n",
       " 'panorama',\n",
       " 'Mindanao',\n",
       " 'Verne',\n",
       " 'policy',\n",
       " 'honey-in-the-sun',\n",
       " 'bromophenol',\n",
       " 'creatures',\n",
       " 'appears',\n",
       " 'Duffy',\n",
       " 'arc',\n",
       " 'vectors',\n",
       " 'Carmody',\n",
       " 'Anglicanism',\n",
       " 'ancestor',\n",
       " 'uttering',\n",
       " 'absorb',\n",
       " 'classifiers',\n",
       " 'blankets',\n",
       " 'vice-president',\n",
       " 'Deller',\n",
       " 'urinary',\n",
       " 'timing',\n",
       " 'Brasstown',\n",
       " 'unquestionably',\n",
       " 'sallow',\n",
       " 'Having',\n",
       " 'lots',\n",
       " 'Tsou',\n",
       " 'gethuh',\n",
       " 'nozzle',\n",
       " 'dad',\n",
       " 'sweepings',\n",
       " 'awed',\n",
       " 'pyschiatrist',\n",
       " 'Hugh',\n",
       " 'layoffs',\n",
       " 'vocalism',\n",
       " 'ethyl',\n",
       " 'Where',\n",
       " 'krautheads',\n",
       " 'BMT',\n",
       " 'intrudes',\n",
       " 'possesses',\n",
       " 'menace',\n",
       " 'anti-American',\n",
       " 'single-foot',\n",
       " 'grasped',\n",
       " 'develop',\n",
       " 'intrusions',\n",
       " 'transmutation',\n",
       " 'actual',\n",
       " 'transversus',\n",
       " 'enrolled',\n",
       " 'surrendering',\n",
       " 'chronicled',\n",
       " 'bone',\n",
       " 'tornadoes',\n",
       " 'punches',\n",
       " 'saturation',\n",
       " 'unsold',\n",
       " '138',\n",
       " 'strenuous',\n",
       " 'buyer',\n",
       " '362',\n",
       " 'juniors',\n",
       " 'Participants',\n",
       " 'STDCR',\n",
       " 'agriculture',\n",
       " 'deliverance',\n",
       " 'craggy',\n",
       " 'Salisbury',\n",
       " 'lifeblood',\n",
       " 'without',\n",
       " 'Schultz',\n",
       " 'Rilly',\n",
       " 'ten-concert',\n",
       " 'long-endurance',\n",
       " 'Tolstoy',\n",
       " 'WAO',\n",
       " 'gotta',\n",
       " 'plunging',\n",
       " 'interpolations',\n",
       " 'divider',\n",
       " 'volumetrically',\n",
       " 'Lamb',\n",
       " 'habe',\n",
       " 'Tenn.',\n",
       " 'toner',\n",
       " 'exporters',\n",
       " 'flautist',\n",
       " 'cross-top',\n",
       " 'notify',\n",
       " 'inauguration',\n",
       " 'best-seller',\n",
       " 'Airline',\n",
       " 'fumes',\n",
       " 'Liston',\n",
       " 'Newbery',\n",
       " 'pricing',\n",
       " 'cloudburst',\n",
       " 'pews',\n",
       " 'necrotic',\n",
       " 'believers',\n",
       " 'Miniver',\n",
       " 'Spokane',\n",
       " 'localisms',\n",
       " 'grunt',\n",
       " 'conscription',\n",
       " 'rimmed',\n",
       " 'heaven',\n",
       " 'individual',\n",
       " 'chaplain',\n",
       " 'co-educational',\n",
       " 'imperfect',\n",
       " 'guaranteed',\n",
       " 'de-iodinating',\n",
       " 'gate-post',\n",
       " 'Anita',\n",
       " 'filmed',\n",
       " 'hairless',\n",
       " 'admires',\n",
       " 'posseman',\n",
       " 'riggers',\n",
       " 'arithmetic',\n",
       " 'Kennan',\n",
       " '1963',\n",
       " 'rheumatic',\n",
       " 'A-1',\n",
       " 'theistic',\n",
       " 'luxurious',\n",
       " 'wastewater',\n",
       " 'past',\n",
       " 'scent',\n",
       " 'Gap',\n",
       " 'Kro^ger',\n",
       " 'Cadet',\n",
       " 'Pont-General',\n",
       " 'heavy-duty',\n",
       " 'Glue',\n",
       " 'Caribbean',\n",
       " 'poses',\n",
       " 'Mining',\n",
       " 'Quotations',\n",
       " 'Bolivia',\n",
       " 'Guests',\n",
       " 'shaded',\n",
       " 'discontinuity',\n",
       " 'thin-lipped',\n",
       " 'system',\n",
       " 'knit',\n",
       " 'undercover',\n",
       " 'verbs',\n",
       " 'apples',\n",
       " 'foreseen',\n",
       " 'exact-size',\n",
       " 'Sapio',\n",
       " 'pleasures',\n",
       " 'skillfulness',\n",
       " 'assaulted',\n",
       " 'Laredo',\n",
       " 'Farrell',\n",
       " 'freezing',\n",
       " 'fullness',\n",
       " 'viewers',\n",
       " 'replenishment',\n",
       " 'Deduction',\n",
       " 'Fujimoto',\n",
       " 'Pax',\n",
       " 'kinda',\n",
       " 'gratification',\n",
       " 'psychosomatic',\n",
       " 'frowning',\n",
       " 'Grab',\n",
       " 'ecumenicists',\n",
       " 'bothers',\n",
       " 'Crusade',\n",
       " 'detention',\n",
       " 'fright',\n",
       " 'technology',\n",
       " 'manure',\n",
       " '1949',\n",
       " 'matters',\n",
       " 'miniature',\n",
       " 'montage',\n",
       " 'turf',\n",
       " 'insolent',\n",
       " 'pre-eminent',\n",
       " 'odd',\n",
       " 'goddamned',\n",
       " 'Southwest',\n",
       " 'enclosure',\n",
       " 'sensibilities',\n",
       " 'debris',\n",
       " 'glorified',\n",
       " 'fasten',\n",
       " 'Continent',\n",
       " 'Shantz',\n",
       " 'Mateo',\n",
       " 'abode',\n",
       " 'fashions',\n",
       " 'jockeying',\n",
       " '240',\n",
       " 'Parisian',\n",
       " 'Gridley',\n",
       " 'Hemenway',\n",
       " 'rolls',\n",
       " 'Boyce',\n",
       " 'Niepce',\n",
       " '365',\n",
       " 'cartoon',\n",
       " 'linger',\n",
       " 'fret',\n",
       " 'campaigns',\n",
       " 'Meistersinger',\n",
       " 'savored',\n",
       " 'marginality',\n",
       " 'Grovers',\n",
       " 'comments',\n",
       " 'sustained',\n",
       " 'gouged',\n",
       " 'showerhead',\n",
       " 'nationalisms',\n",
       " 'discontinuance',\n",
       " 'lime',\n",
       " 'opposing',\n",
       " 'plentiful',\n",
       " 'Farley',\n",
       " 'Livestock',\n",
       " 'meditative',\n",
       " 'sunset',\n",
       " 'bales',\n",
       " 'components',\n",
       " 'Thereupon',\n",
       " 'wrangler',\n",
       " 'Fortier',\n",
       " 'protege',\n",
       " 'billions',\n",
       " 'skating',\n",
       " 'homers',\n",
       " 'performance',\n",
       " 'captures',\n",
       " 'church-state',\n",
       " 'Corsi',\n",
       " 'again',\n",
       " 'entire',\n",
       " 'sterns',\n",
       " 'scavenger',\n",
       " 'catholic',\n",
       " 'sociological',\n",
       " 'adhesives',\n",
       " 'studious',\n",
       " 'setbacks',\n",
       " 'librarian-board',\n",
       " 'duels',\n",
       " 'ass',\n",
       " 'instituted',\n",
       " 'Lounge',\n",
       " 'Krakowiak',\n",
       " 'dancers',\n",
       " 'neo-classicism',\n",
       " 'occupational',\n",
       " 'brow',\n",
       " 'transportation',\n",
       " 'litters',\n",
       " 'infirmary',\n",
       " 'annals',\n",
       " 'Eisler',\n",
       " 'hind',\n",
       " 'ideal',\n",
       " 'Spahnie',\n",
       " 'tetrahalides',\n",
       " 'erected',\n",
       " 'threshing',\n",
       " 'honour',\n",
       " 'Rafer',\n",
       " 'Sailing',\n",
       " 'numbingly',\n",
       " 'paraphrase',\n",
       " 'mythic',\n",
       " 'Bradford',\n",
       " '400',\n",
       " 'inwardly',\n",
       " 'sensing',\n",
       " 'fundamental',\n",
       " 'Ex',\n",
       " 'Heaven',\n",
       " 'Symphony',\n",
       " 'Problems',\n",
       " 'Asia',\n",
       " 'sages',\n",
       " 'Gore',\n",
       " 'talent',\n",
       " 'Or',\n",
       " '608',\n",
       " 'overgrown',\n",
       " 'truer',\n",
       " 'tokenish',\n",
       " 'completed',\n",
       " 'Lumumba',\n",
       " 'provisional',\n",
       " 'Hapgood',\n",
       " 'those',\n",
       " 'windmill',\n",
       " 'powderpuff',\n",
       " 'Buena',\n",
       " 'inquire',\n",
       " 'wracking',\n",
       " 'Multiplication',\n",
       " 'Salle',\n",
       " 'ceremonial',\n",
       " 'aluminum',\n",
       " 'Tradition',\n",
       " 'dies',\n",
       " '5835',\n",
       " 'Guatemala',\n",
       " 'childish',\n",
       " 'Schenk',\n",
       " 'Roberto',\n",
       " 'Colcord',\n",
       " 'Feuermann',\n",
       " 'maht',\n",
       " 'authenticity',\n",
       " 'soutane',\n",
       " 'milling',\n",
       " 'Haaek',\n",
       " 'demure',\n",
       " 'secularists',\n",
       " \"Gre't\",\n",
       " 'abruptness',\n",
       " 'casts',\n",
       " 'storylines',\n",
       " 'wailed',\n",
       " 'homogeneity',\n",
       " 'unrewarding',\n",
       " 'devastated',\n",
       " 'Klan',\n",
       " 'wreak',\n",
       " 'steal',\n",
       " 'nationalism',\n",
       " 'scouts',\n",
       " 'combatants',\n",
       " 'seeped',\n",
       " 'trigger',\n",
       " 'dreadfully',\n",
       " 'installments',\n",
       " 'trans',\n",
       " 'Practice',\n",
       " \"We're\",\n",
       " 'or',\n",
       " 'stuck',\n",
       " 'bel',\n",
       " 'variations',\n",
       " 'Raphael',\n",
       " 'bookcase',\n",
       " 'consoled',\n",
       " 'shape',\n",
       " 'Quartet',\n",
       " 'dreamer',\n",
       " 'badge',\n",
       " 'acts',\n",
       " 'retires',\n",
       " 'purgatory',\n",
       " '3825',\n",
       " 'frowningly',\n",
       " 'Below',\n",
       " 'Hoffa',\n",
       " 'certificate',\n",
       " 'BY',\n",
       " 'Currys',\n",
       " 'drawing-room',\n",
       " 'Assiniboine',\n",
       " 'missing',\n",
       " 'charms',\n",
       " 'breakin',\n",
       " 'strongrooms',\n",
       " 'Nugent',\n",
       " 'Iglehart',\n",
       " 'Headquarters',\n",
       " 'plaintive',\n",
       " '13.5',\n",
       " 'one-two-three',\n",
       " 'parked',\n",
       " 'owned',\n",
       " 'England-born',\n",
       " 'solipsism',\n",
       " 'Shylock',\n",
       " 'Adirondacks',\n",
       " 'caste',\n",
       " '68',\n",
       " 'guiding',\n",
       " 'snacks',\n",
       " 'dia.',\n",
       " 'Sartre',\n",
       " 'Cherkasov',\n",
       " 'deathly',\n",
       " 'populace',\n",
       " 'extensively',\n",
       " 'Sheila',\n",
       " 'airfields',\n",
       " '1821',\n",
       " 'growing',\n",
       " 'subject',\n",
       " 'discrepancies',\n",
       " 'best-selling',\n",
       " 'pushing',\n",
       " 'checkbook',\n",
       " 'alabaster',\n",
       " 'enthalpy',\n",
       " 'HillsDrive',\n",
       " 'Whittier',\n",
       " 'Committeemen',\n",
       " 'dreadful',\n",
       " 'perceptive',\n",
       " 'AID',\n",
       " 'modified',\n",
       " 'delayed',\n",
       " 'uncovered',\n",
       " 'mysticism',\n",
       " 'costumed',\n",
       " 'identifies',\n",
       " 'sacrifices',\n",
       " 'kraut',\n",
       " 'Sox',\n",
       " 'N',\n",
       " 'trappings',\n",
       " 'bulked',\n",
       " 'prowazwki',\n",
       " 'bough',\n",
       " 'buildup',\n",
       " 'salvation',\n",
       " 'phrasemaking',\n",
       " 'Rundfunkchor',\n",
       " 'genesis',\n",
       " 'brevity',\n",
       " 'Taxation',\n",
       " 'Ghoreyeb',\n",
       " 'brushcut',\n",
       " 'eluded',\n",
       " 'deferred',\n",
       " 'Willie',\n",
       " 'carbonates',\n",
       " 'Sweet-sour',\n",
       " 'chafing',\n",
       " 'peering',\n",
       " 'Housekeeping',\n",
       " 'bath',\n",
       " 'preaching',\n",
       " 'vigorous',\n",
       " 'punitive',\n",
       " 'services',\n",
       " 'swimmers',\n",
       " 'double-crossing',\n",
       " 'glisten',\n",
       " 'weighed',\n",
       " 'incorporation',\n",
       " 'Spector',\n",
       " 'bucks',\n",
       " 'unaccompanied',\n",
       " 'critic',\n",
       " 'restaurant',\n",
       " 'Hills',\n",
       " 'tracked',\n",
       " 'stroked',\n",
       " 'Robby',\n",
       " 'terrorized',\n",
       " '.270',\n",
       " 'matchless',\n",
       " 'respectfully',\n",
       " 'plows',\n",
       " 'papillary',\n",
       " 'nets',\n",
       " 'Jude',\n",
       " 'Miyagi',\n",
       " 'Purple',\n",
       " 'brassy',\n",
       " 'capture',\n",
       " 'slopping',\n",
       " 'Shape',\n",
       " 'recently',\n",
       " 'Hayek',\n",
       " 'blocked',\n",
       " 'civilizations',\n",
       " 'cliffs',\n",
       " 'inferior',\n",
       " 'dooms',\n",
       " 'contrasting',\n",
       " 'commendation',\n",
       " 'roi',\n",
       " 'encephalographic',\n",
       " 'malformations',\n",
       " 'Tennyson',\n",
       " 'takings',\n",
       " 'Nostalgic',\n",
       " 'mad',\n",
       " 'Rush',\n",
       " 'complements',\n",
       " 'spooned',\n",
       " 'gouverne',\n",
       " 'Boxford',\n",
       " 'lush',\n",
       " 'sublimed',\n",
       " '2460',\n",
       " 'Favor',\n",
       " 'axle',\n",
       " 'definable',\n",
       " 'pique',\n",
       " 'Gate',\n",
       " 'Hon',\n",
       " 'ducks',\n",
       " 'greeted',\n",
       " 'absorbency',\n",
       " 'conserve',\n",
       " 'Orthopedic',\n",
       " 'looseness',\n",
       " 'arising',\n",
       " 'So-so',\n",
       " 'prejudicial',\n",
       " 'unmotivated',\n",
       " 'Marlene',\n",
       " 'irrevocably',\n",
       " 'wells',\n",
       " 'floes',\n",
       " 'vaccinating',\n",
       " 'terrestrial',\n",
       " 'Sending',\n",
       " 'nucleotide',\n",
       " 'Raymond',\n",
       " 'simmered',\n",
       " 'Communisn',\n",
       " 'racial',\n",
       " 'relaxed',\n",
       " 'catchers',\n",
       " 'Halfback',\n",
       " 'spinach',\n",
       " 'slung',\n",
       " '710',\n",
       " 'Tarrant',\n",
       " 'evaluative',\n",
       " 'principally',\n",
       " 'raiders',\n",
       " 'Morton',\n",
       " 'Geneticist',\n",
       " 'ultracentrifugation',\n",
       " 'redcoats',\n",
       " 'commercialization',\n",
       " 'speculation',\n",
       " 'Promptly',\n",
       " 'traditionalist',\n",
       " 'Charter',\n",
       " 'Stanbury',\n",
       " 'tossed',\n",
       " 'managements',\n",
       " 'kill',\n",
       " 'suds',\n",
       " 'Passage',\n",
       " 'Safety',\n",
       " 'Hyde',\n",
       " 'plasticity',\n",
       " 'obsoleting',\n",
       " 'reloaded',\n",
       " 'catches',\n",
       " 'accelerate',\n",
       " 'stubbed',\n",
       " 'paragraphs',\n",
       " 'hydride',\n",
       " 'appeased',\n",
       " 'Bankhead',\n",
       " 'citron',\n",
       " 'Siciliana',\n",
       " 'censure',\n",
       " 'controlled',\n",
       " 'intimidation',\n",
       " 'women',\n",
       " 'justifications',\n",
       " 'manifesting',\n",
       " 'stockbroker',\n",
       " 'departures',\n",
       " 'cane',\n",
       " 'misrelated',\n",
       " 'stretch',\n",
       " 'Receiving',\n",
       " 'Positive',\n",
       " 'Howard',\n",
       " 'home-grown',\n",
       " 'shovel',\n",
       " 'piety',\n",
       " 'hunkered',\n",
       " 'drawer',\n",
       " 'labyrinth',\n",
       " \"c'mon\",\n",
       " '260',\n",
       " 'gambling',\n",
       " 'unfathomable',\n",
       " '92',\n",
       " 'atypical',\n",
       " 'Bentleys',\n",
       " 'circles',\n",
       " 'assembling',\n",
       " 'collar',\n",
       " 'twenties',\n",
       " 'treatments',\n",
       " 'underbedding',\n",
       " 'chowders',\n",
       " 'smashing',\n",
       " 'gripping',\n",
       " 'Milties',\n",
       " 'Saloon',\n",
       " 'lap',\n",
       " 'compassion',\n",
       " 'Espagnol',\n",
       " 'fatality',\n",
       " 'transplantable',\n",
       " 'junctures',\n",
       " 'Greek',\n",
       " 'thrust',\n",
       " 'McAuliffe',\n",
       " 'Meaning',\n",
       " 're',\n",
       " 'conscience',\n",
       " 'algebraically',\n",
       " 'cartoonist',\n",
       " 'Sharp',\n",
       " 'allusion',\n",
       " 'conveniences',\n",
       " 'swooping',\n",
       " '130',\n",
       " 'resifted',\n",
       " 'Recognizing',\n",
       " 'Tim',\n",
       " 'Attempts',\n",
       " 'splendidly',\n",
       " 'Dante',\n",
       " 'o',\n",
       " 'interlude',\n",
       " 'proclamations',\n",
       " 'netted',\n",
       " 'Duncan',\n",
       " 'Benzell',\n",
       " 'Mathues',\n",
       " 'milks',\n",
       " 'Export',\n",
       " 'imperfection',\n",
       " 'calls',\n",
       " '325',\n",
       " 'fantastic',\n",
       " 'doorways',\n",
       " 'Mailer',\n",
       " 'social',\n",
       " 'sexes',\n",
       " 'Vast',\n",
       " 'Langsdorf',\n",
       " 'anchorite',\n",
       " 'Crockett',\n",
       " 'lesbians',\n",
       " 'slogans',\n",
       " 'erecting',\n",
       " 'Talk',\n",
       " 'presumption',\n",
       " 'Follows',\n",
       " 'Bellows',\n",
       " 'grizzled',\n",
       " 'whacked',\n",
       " 'Pride-Venus',\n",
       " 'draining',\n",
       " 'Yeah',\n",
       " 'obey',\n",
       " 'slow-baked',\n",
       " 'Neurenschatz',\n",
       " 'stays',\n",
       " 'fishing',\n",
       " 'Barth',\n",
       " 'freshmen',\n",
       " 'dogma',\n",
       " 'transaction',\n",
       " 'Looky',\n",
       " 'stockpiling',\n",
       " 'emptiness',\n",
       " 'dissatisfied',\n",
       " 'towns',\n",
       " 'flock',\n",
       " 'Young',\n",
       " 'hyperbolically',\n",
       " 'Lissa',\n",
       " 'overlooked',\n",
       " 'undependable',\n",
       " 'Ventilation',\n",
       " 'quarrelsome',\n",
       " 'Resident',\n",
       " 'sentimentality',\n",
       " 'flowered',\n",
       " 'Gaining',\n",
       " 'straws',\n",
       " 'documented',\n",
       " 'Handlers',\n",
       " 'radionic',\n",
       " 'tribesmen',\n",
       " 'combination',\n",
       " 'presidency',\n",
       " 'conceived',\n",
       " 'railroads',\n",
       " 'Juan',\n",
       " 'unload',\n",
       " 'inhalation',\n",
       " '102285000',\n",
       " 'introduces',\n",
       " 'shoving',\n",
       " 'capability',\n",
       " 'Alusik',\n",
       " 'cafeterias',\n",
       " 'predisposed',\n",
       " 'cooky',\n",
       " 'bookings',\n",
       " 'Odessa',\n",
       " 'round-the-clock',\n",
       " 'recommendation',\n",
       " 'Tell',\n",
       " 'neutralization',\n",
       " 'tarpaulin',\n",
       " 'sunlight',\n",
       " 'cocopalm',\n",
       " 'Sweeping',\n",
       " 'impulsive',\n",
       " 'superstitions',\n",
       " 'school',\n",
       " 'swap',\n",
       " 'slovenly',\n",
       " 'Lunch',\n",
       " 'One-Leg',\n",
       " 'drowning',\n",
       " 'behalf',\n",
       " '0.075',\n",
       " 'hotel',\n",
       " 'shu-tt',\n",
       " 'monumental',\n",
       " 'Mallinckrodt',\n",
       " 'expeditions',\n",
       " 'Cap',\n",
       " 'vivo',\n",
       " 'prize',\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([26984])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana = torch.LongTensor([word2index['banana']])\n",
    "banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3690, -0.7360]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana_embed_c = model.embedding_center(banana)\n",
    "banana_embed_o = model.embedding_outside(banana)\n",
    "banana_embed   = (banana_embed_c + banana_embed_o) / 2\n",
    "banana_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9106, -1.2587]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana_embed_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "        \n",
    "    word = torch.LongTensor([word2index[word]])\n",
    "    \n",
    "    embed_c = model.embedding_center(word)\n",
    "    embed_o = model.embedding_outside(word)\n",
    "    embed   = (embed_c + embed_o) / 2\n",
    "    \n",
    "    return embed[0][0].item(), embed[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6589943170547485, 0.5398509502410889)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed('fruit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28687793016433716, -0.746550440788269)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.4033914804458618, 0.23445473611354828)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.36900272965431213, -0.7359731197357178)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed('banana')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.36900272965431213, -0.7359731197357178)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana = get_embed('banana')\n",
    "banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6589943170547485, 0.5398509502410889)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit = get_embed('fruit')\n",
    "fruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.45426058769226074, -0.19747182726860046)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk = get_embed('<UNK>')\n",
    "unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3129573536076009"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(banana) @ np.array(unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7674269748659595\n",
      "-0.21978110530094994\n"
     ]
    }
   ],
   "source": [
    "#more formally is to divide by its norm\n",
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_a = np.linalg.norm(A)\n",
    "    norm_b = np.linalg.norm(B)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity\n",
    "\n",
    "print(cosine_similarity(np.array(banana), np.array(unk)))\n",
    "print(cosine_similarity(np.array(banana), np.array(fruit)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'skipgram_model.pkl'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export word2index and index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word2index, open('skipgram_word2index.pkl', 'wb'))\n",
    "pickle.dump(index2word, open('skipgram_index2word.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
